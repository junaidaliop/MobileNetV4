
==================================================
Model: MobileNetV4HybridLarge
==================================================
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1        [128, 24, 191, 191]             648
       BatchNorm2d-2        [128, 24, 191, 191]              48
              GELU-3        [128, 24, 191, 191]               0
     Conv2DBNBlock-4        [128, 24, 191, 191]               0
            Conv2d-5          [128, 96, 96, 96]          20,736
       BatchNorm2d-6          [128, 96, 96, 96]             192
              GELU-7          [128, 96, 96, 96]               0
            Conv2d-8          [128, 48, 96, 96]           4,608
       BatchNorm2d-9          [128, 48, 96, 96]              96
FusedInvertedBottleneckBlock-10          [128, 48, 96, 96]               0
           Conv2d-11          [128, 48, 96, 96]             432
      BatchNorm2d-12          [128, 48, 96, 96]              96
           Conv2d-13         [128, 192, 96, 96]           9,216
      BatchNorm2d-14         [128, 192, 96, 96]             384
            ReLU6-15         [128, 192, 96, 96]               0
           Conv2d-16         [128, 192, 48, 48]           4,800
      BatchNorm2d-17         [128, 192, 48, 48]             384
            ReLU6-18         [128, 192, 48, 48]               0
           Conv2d-19          [128, 96, 48, 48]          18,432
      BatchNorm2d-20          [128, 96, 48, 48]             192
   MNV4LayerScale-21          [128, 96, 48, 48]               0
UniversalInvertedBottleneckBlock-22          [128, 96, 48, 48]               0
           Conv2d-23          [128, 96, 48, 48]             864
      BatchNorm2d-24          [128, 96, 48, 48]             192
           Conv2d-25         [128, 384, 48, 48]          36,864
      BatchNorm2d-26         [128, 384, 48, 48]             768
            ReLU6-27         [128, 384, 48, 48]               0
           Conv2d-28         [128, 384, 48, 48]           3,456
      BatchNorm2d-29         [128, 384, 48, 48]             768
            ReLU6-30         [128, 384, 48, 48]               0
           Conv2d-31          [128, 96, 48, 48]          36,864
      BatchNorm2d-32          [128, 96, 48, 48]             192
   MNV4LayerScale-33          [128, 96, 48, 48]               0
UniversalInvertedBottleneckBlock-34          [128, 96, 48, 48]               0
           Conv2d-35          [128, 96, 48, 48]             864
      BatchNorm2d-36          [128, 96, 48, 48]             192
           Conv2d-37         [128, 384, 48, 48]          36,864
      BatchNorm2d-38         [128, 384, 48, 48]             768
            ReLU6-39         [128, 384, 48, 48]               0
           Conv2d-40         [128, 384, 24, 24]           9,600
      BatchNorm2d-41         [128, 384, 24, 24]             768
            ReLU6-42         [128, 384, 24, 24]               0
           Conv2d-43         [128, 192, 24, 24]          73,728
      BatchNorm2d-44         [128, 192, 24, 24]             384
   MNV4LayerScale-45         [128, 192, 24, 24]               0
UniversalInvertedBottleneckBlock-46         [128, 192, 24, 24]               0
           Conv2d-47         [128, 192, 24, 24]           1,728
      BatchNorm2d-48         [128, 192, 24, 24]             384
           Conv2d-49         [128, 768, 24, 24]         147,456
      BatchNorm2d-50         [128, 768, 24, 24]           1,536
            ReLU6-51         [128, 768, 24, 24]               0
           Conv2d-52         [128, 768, 24, 24]           6,912
      BatchNorm2d-53         [128, 768, 24, 24]           1,536
            ReLU6-54         [128, 768, 24, 24]               0
           Conv2d-55         [128, 192, 24, 24]         147,456
      BatchNorm2d-56         [128, 192, 24, 24]             384
   MNV4LayerScale-57         [128, 192, 24, 24]               0
UniversalInvertedBottleneckBlock-58         [128, 192, 24, 24]               0
           Conv2d-59         [128, 192, 24, 24]           1,728
      BatchNorm2d-60         [128, 192, 24, 24]             384
           Conv2d-61         [128, 768, 24, 24]         147,456
      BatchNorm2d-62         [128, 768, 24, 24]           1,536
            ReLU6-63         [128, 768, 24, 24]               0
           Conv2d-64         [128, 768, 24, 24]           6,912
      BatchNorm2d-65         [128, 768, 24, 24]           1,536
            ReLU6-66         [128, 768, 24, 24]               0
           Conv2d-67         [128, 192, 24, 24]         147,456
      BatchNorm2d-68         [128, 192, 24, 24]             384
   MNV4LayerScale-69         [128, 192, 24, 24]               0
UniversalInvertedBottleneckBlock-70         [128, 192, 24, 24]               0
           Conv2d-71         [128, 192, 24, 24]           1,728
      BatchNorm2d-72         [128, 192, 24, 24]             384
           Conv2d-73         [128, 768, 24, 24]         147,456
      BatchNorm2d-74         [128, 768, 24, 24]           1,536
            ReLU6-75         [128, 768, 24, 24]               0
           Conv2d-76         [128, 768, 24, 24]           6,912
      BatchNorm2d-77         [128, 768, 24, 24]           1,536
            ReLU6-78         [128, 768, 24, 24]               0
           Conv2d-79         [128, 192, 24, 24]         147,456
      BatchNorm2d-80         [128, 192, 24, 24]             384
   MNV4LayerScale-81         [128, 192, 24, 24]               0
UniversalInvertedBottleneckBlock-82         [128, 192, 24, 24]               0
           Conv2d-83         [128, 192, 24, 24]           1,728
      BatchNorm2d-84         [128, 192, 24, 24]             384
           Conv2d-85         [128, 768, 24, 24]         147,456
      BatchNorm2d-86         [128, 768, 24, 24]           1,536
            ReLU6-87         [128, 768, 24, 24]               0
           Conv2d-88         [128, 768, 24, 24]          19,200
      BatchNorm2d-89         [128, 768, 24, 24]           1,536
            ReLU6-90         [128, 768, 24, 24]               0
           Conv2d-91         [128, 192, 24, 24]         147,456
      BatchNorm2d-92         [128, 192, 24, 24]             384
   MNV4LayerScale-93         [128, 192, 24, 24]               0
UniversalInvertedBottleneckBlock-94         [128, 192, 24, 24]               0
           Conv2d-95         [128, 192, 24, 24]           4,800
      BatchNorm2d-96         [128, 192, 24, 24]             384
           Conv2d-97         [128, 768, 24, 24]         147,456
      BatchNorm2d-98         [128, 768, 24, 24]           1,536
            ReLU6-99         [128, 768, 24, 24]               0
          Conv2d-100         [128, 768, 24, 24]           6,912
     BatchNorm2d-101         [128, 768, 24, 24]           1,536
           ReLU6-102         [128, 768, 24, 24]               0
          Conv2d-103         [128, 192, 24, 24]         147,456
     BatchNorm2d-104         [128, 192, 24, 24]             384
  MNV4LayerScale-105         [128, 192, 24, 24]               0
UniversalInvertedBottleneckBlock-106         [128, 192, 24, 24]               0
          Conv2d-107         [128, 192, 24, 24]           4,800
     BatchNorm2d-108         [128, 192, 24, 24]             384
          Conv2d-109         [128, 768, 24, 24]         147,456
     BatchNorm2d-110         [128, 768, 24, 24]           1,536
           ReLU6-111         [128, 768, 24, 24]               0
          Conv2d-112         [128, 768, 24, 24]           6,912
     BatchNorm2d-113         [128, 768, 24, 24]           1,536
           ReLU6-114         [128, 768, 24, 24]               0
          Conv2d-115         [128, 192, 24, 24]         147,456
     BatchNorm2d-116         [128, 192, 24, 24]             384
  MNV4LayerScale-117         [128, 192, 24, 24]               0
UniversalInvertedBottleneckBlock-118         [128, 192, 24, 24]               0
     BatchNorm2d-119         [128, 192, 24, 24]             384
        Identity-120         [128, 192, 24, 24]               0
     BatchNorm2d-121         [128, 192, 24, 24]             384
          Conv2d-122         [128, 384, 24, 24]          73,728
          Conv2d-123         [128, 384, 12, 12]           3,456
     BatchNorm2d-124         [128, 384, 12, 12]             768
          Conv2d-125         [128, 384, 12, 12]         147,456
          Conv2d-126         [128, 384, 12, 12]           3,456
     BatchNorm2d-127         [128, 384, 12, 12]             768
          Conv2d-128         [128, 384, 12, 12]         147,456
         Dropout-129         [128, 8, 576, 144]               0
        Identity-130         [128, 384, 24, 24]               0
          Conv2d-131         [128, 192, 24, 24]          73,728
OptimizedMultiQueryAttentionLayerWithDownSampling-132         [128, 192, 24, 24]               0
  MNV4LayerScale-133         [128, 192, 24, 24]               0
MultiHeadSelfAttentionBlock-134         [128, 192, 24, 24]               0
          Conv2d-135         [128, 192, 24, 24]           4,800
     BatchNorm2d-136         [128, 192, 24, 24]             384
          Conv2d-137         [128, 768, 24, 24]         147,456
     BatchNorm2d-138         [128, 768, 24, 24]           1,536
           ReLU6-139         [128, 768, 24, 24]               0
          Conv2d-140         [128, 768, 24, 24]           6,912
     BatchNorm2d-141         [128, 768, 24, 24]           1,536
           ReLU6-142         [128, 768, 24, 24]               0
          Conv2d-143         [128, 192, 24, 24]         147,456
     BatchNorm2d-144         [128, 192, 24, 24]             384
  MNV4LayerScale-145         [128, 192, 24, 24]               0
UniversalInvertedBottleneckBlock-146         [128, 192, 24, 24]               0
     BatchNorm2d-147         [128, 192, 24, 24]             384
        Identity-148         [128, 192, 24, 24]               0
     BatchNorm2d-149         [128, 192, 24, 24]             384
          Conv2d-150         [128, 384, 24, 24]          73,728
          Conv2d-151         [128, 384, 12, 12]           3,456
     BatchNorm2d-152         [128, 384, 12, 12]             768
          Conv2d-153         [128, 384, 12, 12]         147,456
          Conv2d-154         [128, 384, 12, 12]           3,456
     BatchNorm2d-155         [128, 384, 12, 12]             768
          Conv2d-156         [128, 384, 12, 12]         147,456
         Dropout-157         [128, 8, 576, 144]               0
        Identity-158         [128, 384, 24, 24]               0
          Conv2d-159         [128, 192, 24, 24]          73,728
OptimizedMultiQueryAttentionLayerWithDownSampling-160         [128, 192, 24, 24]               0
  MNV4LayerScale-161         [128, 192, 24, 24]               0
MultiHeadSelfAttentionBlock-162         [128, 192, 24, 24]               0
          Conv2d-163         [128, 192, 24, 24]           4,800
     BatchNorm2d-164         [128, 192, 24, 24]             384
          Conv2d-165         [128, 768, 24, 24]         147,456
     BatchNorm2d-166         [128, 768, 24, 24]           1,536
           ReLU6-167         [128, 768, 24, 24]               0
          Conv2d-168         [128, 768, 24, 24]           6,912
     BatchNorm2d-169         [128, 768, 24, 24]           1,536
           ReLU6-170         [128, 768, 24, 24]               0
          Conv2d-171         [128, 192, 24, 24]         147,456
     BatchNorm2d-172         [128, 192, 24, 24]             384
  MNV4LayerScale-173         [128, 192, 24, 24]               0
UniversalInvertedBottleneckBlock-174         [128, 192, 24, 24]               0
     BatchNorm2d-175         [128, 192, 24, 24]             384
        Identity-176         [128, 192, 24, 24]               0
     BatchNorm2d-177         [128, 192, 24, 24]             384
          Conv2d-178         [128, 384, 24, 24]          73,728
          Conv2d-179         [128, 384, 12, 12]           3,456
     BatchNorm2d-180         [128, 384, 12, 12]             768
          Conv2d-181         [128, 384, 12, 12]         147,456
          Conv2d-182         [128, 384, 12, 12]           3,456
     BatchNorm2d-183         [128, 384, 12, 12]             768
          Conv2d-184         [128, 384, 12, 12]         147,456
         Dropout-185         [128, 8, 576, 144]               0
        Identity-186         [128, 384, 24, 24]               0
          Conv2d-187         [128, 192, 24, 24]          73,728
OptimizedMultiQueryAttentionLayerWithDownSampling-188         [128, 192, 24, 24]               0
  MNV4LayerScale-189         [128, 192, 24, 24]               0
MultiHeadSelfAttentionBlock-190         [128, 192, 24, 24]               0
          Conv2d-191         [128, 192, 24, 24]           4,800
     BatchNorm2d-192         [128, 192, 24, 24]             384
          Conv2d-193         [128, 768, 24, 24]         147,456
     BatchNorm2d-194         [128, 768, 24, 24]           1,536
           ReLU6-195         [128, 768, 24, 24]               0
          Conv2d-196         [128, 768, 24, 24]           6,912
     BatchNorm2d-197         [128, 768, 24, 24]           1,536
           ReLU6-198         [128, 768, 24, 24]               0
          Conv2d-199         [128, 192, 24, 24]         147,456
     BatchNorm2d-200         [128, 192, 24, 24]             384
  MNV4LayerScale-201         [128, 192, 24, 24]               0
UniversalInvertedBottleneckBlock-202         [128, 192, 24, 24]               0
     BatchNorm2d-203         [128, 192, 24, 24]             384
        Identity-204         [128, 192, 24, 24]               0
     BatchNorm2d-205         [128, 192, 24, 24]             384
          Conv2d-206         [128, 384, 24, 24]          73,728
          Conv2d-207         [128, 384, 12, 12]           3,456
     BatchNorm2d-208         [128, 384, 12, 12]             768
          Conv2d-209         [128, 384, 12, 12]         147,456
          Conv2d-210         [128, 384, 12, 12]           3,456
     BatchNorm2d-211         [128, 384, 12, 12]             768
          Conv2d-212         [128, 384, 12, 12]         147,456
         Dropout-213         [128, 8, 576, 144]               0
        Identity-214         [128, 384, 24, 24]               0
          Conv2d-215         [128, 192, 24, 24]          73,728
OptimizedMultiQueryAttentionLayerWithDownSampling-216         [128, 192, 24, 24]               0
  MNV4LayerScale-217         [128, 192, 24, 24]               0
MultiHeadSelfAttentionBlock-218         [128, 192, 24, 24]               0
          Conv2d-219         [128, 192, 24, 24]           1,728
     BatchNorm2d-220         [128, 192, 24, 24]             384
          Conv2d-221         [128, 768, 24, 24]         147,456
     BatchNorm2d-222         [128, 768, 24, 24]           1,536
           ReLU6-223         [128, 768, 24, 24]               0
          Conv2d-224         [128, 192, 24, 24]         147,456
     BatchNorm2d-225         [128, 192, 24, 24]             384
  MNV4LayerScale-226         [128, 192, 24, 24]               0
UniversalInvertedBottleneckBlock-227         [128, 192, 24, 24]               0
          Conv2d-228         [128, 192, 24, 24]           4,800
     BatchNorm2d-229         [128, 192, 24, 24]             384
          Conv2d-230         [128, 768, 24, 24]         147,456
     BatchNorm2d-231         [128, 768, 24, 24]           1,536
           ReLU6-232         [128, 768, 24, 24]               0
          Conv2d-233         [128, 768, 12, 12]          19,200
     BatchNorm2d-234         [128, 768, 12, 12]           1,536
           ReLU6-235         [128, 768, 12, 12]               0
          Conv2d-236         [128, 512, 12, 12]         393,216
     BatchNorm2d-237         [128, 512, 12, 12]           1,024
  MNV4LayerScale-238         [128, 512, 12, 12]               0
UniversalInvertedBottleneckBlock-239         [128, 512, 12, 12]               0
          Conv2d-240         [128, 512, 12, 12]          12,800
     BatchNorm2d-241         [128, 512, 12, 12]           1,024
          Conv2d-242        [128, 2048, 12, 12]       1,048,576
     BatchNorm2d-243        [128, 2048, 12, 12]           4,096
           ReLU6-244        [128, 2048, 12, 12]               0
          Conv2d-245        [128, 2048, 12, 12]          51,200
     BatchNorm2d-246        [128, 2048, 12, 12]           4,096
           ReLU6-247        [128, 2048, 12, 12]               0
          Conv2d-248         [128, 512, 12, 12]       1,048,576
     BatchNorm2d-249         [128, 512, 12, 12]           1,024
  MNV4LayerScale-250         [128, 512, 12, 12]               0
UniversalInvertedBottleneckBlock-251         [128, 512, 12, 12]               0
          Conv2d-252         [128, 512, 12, 12]          12,800
     BatchNorm2d-253         [128, 512, 12, 12]           1,024
          Conv2d-254        [128, 2048, 12, 12]       1,048,576
     BatchNorm2d-255        [128, 2048, 12, 12]           4,096
           ReLU6-256        [128, 2048, 12, 12]               0
          Conv2d-257        [128, 2048, 12, 12]          51,200
     BatchNorm2d-258        [128, 2048, 12, 12]           4,096
           ReLU6-259        [128, 2048, 12, 12]               0
          Conv2d-260         [128, 512, 12, 12]       1,048,576
     BatchNorm2d-261         [128, 512, 12, 12]           1,024
  MNV4LayerScale-262         [128, 512, 12, 12]               0
UniversalInvertedBottleneckBlock-263         [128, 512, 12, 12]               0
          Conv2d-264         [128, 512, 12, 12]          12,800
     BatchNorm2d-265         [128, 512, 12, 12]           1,024
          Conv2d-266        [128, 2048, 12, 12]       1,048,576
     BatchNorm2d-267        [128, 2048, 12, 12]           4,096
           ReLU6-268        [128, 2048, 12, 12]               0
          Conv2d-269        [128, 2048, 12, 12]          51,200
     BatchNorm2d-270        [128, 2048, 12, 12]           4,096
           ReLU6-271        [128, 2048, 12, 12]               0
          Conv2d-272         [128, 512, 12, 12]       1,048,576
     BatchNorm2d-273         [128, 512, 12, 12]           1,024
  MNV4LayerScale-274         [128, 512, 12, 12]               0
UniversalInvertedBottleneckBlock-275         [128, 512, 12, 12]               0
          Conv2d-276         [128, 512, 12, 12]          12,800
     BatchNorm2d-277         [128, 512, 12, 12]           1,024
          Conv2d-278        [128, 2048, 12, 12]       1,048,576
     BatchNorm2d-279        [128, 2048, 12, 12]           4,096
           ReLU6-280        [128, 2048, 12, 12]               0
          Conv2d-281         [128, 512, 12, 12]       1,048,576
     BatchNorm2d-282         [128, 512, 12, 12]           1,024
  MNV4LayerScale-283         [128, 512, 12, 12]               0
UniversalInvertedBottleneckBlock-284         [128, 512, 12, 12]               0
          Conv2d-285         [128, 512, 12, 12]          12,800
     BatchNorm2d-286         [128, 512, 12, 12]           1,024
          Conv2d-287        [128, 2048, 12, 12]       1,048,576
     BatchNorm2d-288        [128, 2048, 12, 12]           4,096
           ReLU6-289        [128, 2048, 12, 12]               0
          Conv2d-290        [128, 2048, 12, 12]          18,432
     BatchNorm2d-291        [128, 2048, 12, 12]           4,096
           ReLU6-292        [128, 2048, 12, 12]               0
          Conv2d-293         [128, 512, 12, 12]       1,048,576
     BatchNorm2d-294         [128, 512, 12, 12]           1,024
  MNV4LayerScale-295         [128, 512, 12, 12]               0
UniversalInvertedBottleneckBlock-296         [128, 512, 12, 12]               0
          Conv2d-297         [128, 512, 12, 12]          12,800
     BatchNorm2d-298         [128, 512, 12, 12]           1,024
          Conv2d-299        [128, 2048, 12, 12]       1,048,576
     BatchNorm2d-300        [128, 2048, 12, 12]           4,096
           ReLU6-301        [128, 2048, 12, 12]               0
          Conv2d-302         [128, 512, 12, 12]       1,048,576
     BatchNorm2d-303         [128, 512, 12, 12]           1,024
  MNV4LayerScale-304         [128, 512, 12, 12]               0
UniversalInvertedBottleneckBlock-305         [128, 512, 12, 12]               0
          Conv2d-306         [128, 512, 12, 12]          12,800
     BatchNorm2d-307         [128, 512, 12, 12]           1,024
          Conv2d-308        [128, 2048, 12, 12]       1,048,576
     BatchNorm2d-309        [128, 2048, 12, 12]           4,096
           ReLU6-310        [128, 2048, 12, 12]               0
          Conv2d-311         [128, 512, 12, 12]       1,048,576
     BatchNorm2d-312         [128, 512, 12, 12]           1,024
  MNV4LayerScale-313         [128, 512, 12, 12]               0
UniversalInvertedBottleneckBlock-314         [128, 512, 12, 12]               0
          Conv2d-315         [128, 512, 12, 12]          12,800
     BatchNorm2d-316         [128, 512, 12, 12]           1,024
          Conv2d-317        [128, 2048, 12, 12]       1,048,576
     BatchNorm2d-318        [128, 2048, 12, 12]           4,096
           ReLU6-319        [128, 2048, 12, 12]               0
          Conv2d-320        [128, 2048, 12, 12]          18,432
     BatchNorm2d-321        [128, 2048, 12, 12]           4,096
           ReLU6-322        [128, 2048, 12, 12]               0
          Conv2d-323         [128, 512, 12, 12]       1,048,576
     BatchNorm2d-324         [128, 512, 12, 12]           1,024
  MNV4LayerScale-325         [128, 512, 12, 12]               0
UniversalInvertedBottleneckBlock-326         [128, 512, 12, 12]               0
          Conv2d-327         [128, 512, 12, 12]          12,800
     BatchNorm2d-328         [128, 512, 12, 12]           1,024
          Conv2d-329        [128, 2048, 12, 12]       1,048,576
     BatchNorm2d-330        [128, 2048, 12, 12]           4,096
           ReLU6-331        [128, 2048, 12, 12]               0
          Conv2d-332        [128, 2048, 12, 12]          51,200
     BatchNorm2d-333        [128, 2048, 12, 12]           4,096
           ReLU6-334        [128, 2048, 12, 12]               0
          Conv2d-335         [128, 512, 12, 12]       1,048,576
     BatchNorm2d-336         [128, 512, 12, 12]           1,024
  MNV4LayerScale-337         [128, 512, 12, 12]               0
UniversalInvertedBottleneckBlock-338         [128, 512, 12, 12]               0
     BatchNorm2d-339         [128, 512, 12, 12]           1,024
        Identity-340         [128, 512, 12, 12]               0
     BatchNorm2d-341         [128, 512, 12, 12]           1,024
          Conv2d-342         [128, 512, 12, 12]         262,144
        Identity-343         [128, 512, 12, 12]               0
     BatchNorm2d-344         [128, 512, 12, 12]           1,024
          Conv2d-345         [128, 512, 12, 12]         262,144
        Identity-346         [128, 512, 12, 12]               0
     BatchNorm2d-347         [128, 512, 12, 12]           1,024
          Conv2d-348         [128, 512, 12, 12]         262,144
         Dropout-349         [128, 8, 144, 144]               0
        Identity-350         [128, 512, 12, 12]               0
          Conv2d-351         [128, 512, 12, 12]         262,144
OptimizedMultiQueryAttentionLayerWithDownSampling-352         [128, 512, 12, 12]               0
  MNV4LayerScale-353         [128, 512, 12, 12]               0
MultiHeadSelfAttentionBlock-354         [128, 512, 12, 12]               0
          Conv2d-355         [128, 512, 12, 12]          12,800
     BatchNorm2d-356         [128, 512, 12, 12]           1,024
          Conv2d-357        [128, 2048, 12, 12]       1,048,576
     BatchNorm2d-358        [128, 2048, 12, 12]           4,096
           ReLU6-359        [128, 2048, 12, 12]               0
          Conv2d-360         [128, 512, 12, 12]       1,048,576
     BatchNorm2d-361         [128, 512, 12, 12]           1,024
  MNV4LayerScale-362         [128, 512, 12, 12]               0
UniversalInvertedBottleneckBlock-363         [128, 512, 12, 12]               0
     BatchNorm2d-364         [128, 512, 12, 12]           1,024
        Identity-365         [128, 512, 12, 12]               0
     BatchNorm2d-366         [128, 512, 12, 12]           1,024
          Conv2d-367         [128, 512, 12, 12]         262,144
        Identity-368         [128, 512, 12, 12]               0
     BatchNorm2d-369         [128, 512, 12, 12]           1,024
          Conv2d-370         [128, 512, 12, 12]         262,144
        Identity-371         [128, 512, 12, 12]               0
     BatchNorm2d-372         [128, 512, 12, 12]           1,024
          Conv2d-373         [128, 512, 12, 12]         262,144
         Dropout-374         [128, 8, 144, 144]               0
        Identity-375         [128, 512, 12, 12]               0
          Conv2d-376         [128, 512, 12, 12]         262,144
OptimizedMultiQueryAttentionLayerWithDownSampling-377         [128, 512, 12, 12]               0
  MNV4LayerScale-378         [128, 512, 12, 12]               0
MultiHeadSelfAttentionBlock-379         [128, 512, 12, 12]               0
          Conv2d-380         [128, 512, 12, 12]          12,800
     BatchNorm2d-381         [128, 512, 12, 12]           1,024
          Conv2d-382        [128, 2048, 12, 12]       1,048,576
     BatchNorm2d-383        [128, 2048, 12, 12]           4,096
           ReLU6-384        [128, 2048, 12, 12]               0
          Conv2d-385         [128, 512, 12, 12]       1,048,576
     BatchNorm2d-386         [128, 512, 12, 12]           1,024
  MNV4LayerScale-387         [128, 512, 12, 12]               0
UniversalInvertedBottleneckBlock-388         [128, 512, 12, 12]               0
     BatchNorm2d-389         [128, 512, 12, 12]           1,024
        Identity-390         [128, 512, 12, 12]               0
     BatchNorm2d-391         [128, 512, 12, 12]           1,024
          Conv2d-392         [128, 512, 12, 12]         262,144
        Identity-393         [128, 512, 12, 12]               0
     BatchNorm2d-394         [128, 512, 12, 12]           1,024
          Conv2d-395         [128, 512, 12, 12]         262,144
        Identity-396         [128, 512, 12, 12]               0
     BatchNorm2d-397         [128, 512, 12, 12]           1,024
          Conv2d-398         [128, 512, 12, 12]         262,144
         Dropout-399         [128, 8, 144, 144]               0
        Identity-400         [128, 512, 12, 12]               0
          Conv2d-401         [128, 512, 12, 12]         262,144
OptimizedMultiQueryAttentionLayerWithDownSampling-402         [128, 512, 12, 12]               0
  MNV4LayerScale-403         [128, 512, 12, 12]               0
MultiHeadSelfAttentionBlock-404         [128, 512, 12, 12]               0
          Conv2d-405         [128, 512, 12, 12]          12,800
     BatchNorm2d-406         [128, 512, 12, 12]           1,024
          Conv2d-407        [128, 2048, 12, 12]       1,048,576
     BatchNorm2d-408        [128, 2048, 12, 12]           4,096
           ReLU6-409        [128, 2048, 12, 12]               0
          Conv2d-410         [128, 512, 12, 12]       1,048,576
     BatchNorm2d-411         [128, 512, 12, 12]           1,024
  MNV4LayerScale-412         [128, 512, 12, 12]               0
UniversalInvertedBottleneckBlock-413         [128, 512, 12, 12]               0
     BatchNorm2d-414         [128, 512, 12, 12]           1,024
        Identity-415         [128, 512, 12, 12]               0
     BatchNorm2d-416         [128, 512, 12, 12]           1,024
          Conv2d-417         [128, 512, 12, 12]         262,144
        Identity-418         [128, 512, 12, 12]               0
     BatchNorm2d-419         [128, 512, 12, 12]           1,024
          Conv2d-420         [128, 512, 12, 12]         262,144
        Identity-421         [128, 512, 12, 12]               0
     BatchNorm2d-422         [128, 512, 12, 12]           1,024
          Conv2d-423         [128, 512, 12, 12]         262,144
         Dropout-424         [128, 8, 144, 144]               0
        Identity-425         [128, 512, 12, 12]               0
          Conv2d-426         [128, 512, 12, 12]         262,144
OptimizedMultiQueryAttentionLayerWithDownSampling-427         [128, 512, 12, 12]               0
  MNV4LayerScale-428         [128, 512, 12, 12]               0
MultiHeadSelfAttentionBlock-429         [128, 512, 12, 12]               0
          Conv2d-430         [128, 512, 12, 12]          12,800
     BatchNorm2d-431         [128, 512, 12, 12]           1,024
          Conv2d-432        [128, 2048, 12, 12]       1,048,576
     BatchNorm2d-433        [128, 2048, 12, 12]           4,096
           ReLU6-434        [128, 2048, 12, 12]               0
          Conv2d-435         [128, 512, 12, 12]       1,048,576
     BatchNorm2d-436         [128, 512, 12, 12]           1,024
  MNV4LayerScale-437         [128, 512, 12, 12]               0
UniversalInvertedBottleneckBlock-438         [128, 512, 12, 12]               0
          Conv2d-439         [128, 960, 12, 12]         491,520
     BatchNorm2d-440         [128, 960, 12, 12]           1,920
            GELU-441         [128, 960, 12, 12]               0
   Conv2DBNBlock-442         [128, 960, 12, 12]               0
AdaptiveAvgPool2d-443           [128, 960, 1, 1]               0
GlobalPoolingBlock-444           [128, 960, 1, 1]               0
          Conv2d-445          [128, 1280, 1, 1]       1,228,800
     BatchNorm2d-446          [128, 1280, 1, 1]           2,560
            GELU-447          [128, 1280, 1, 1]               0
   Conv2DBNBlock-448          [128, 1280, 1, 1]               0
       MobileNet-449          [128, 1280, 1, 1]               0
          Conv2d-450           [128, 100, 1, 1]         128,100
         Flatten-451                 [128, 100]               0
================================================================
Total params: 39,571,084
Trainable params: 39,571,084
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 216.00
Forward/backward pass size (MB): 95516.41
Params size (MB): 150.95
Estimated Total Size (MB): 95883.37
----------------------------------------------------------------

Detailed layer shapes:
features.layers.0.conv: torch.Size([128, 24, 191, 191])
features.layers.0.bn: torch.Size([128, 24, 191, 191])
features.layers.0.activation_layer: torch.Size([128, 24, 191, 191])
features.layers.1.fused_conv: torch.Size([128, 96, 96, 96])
features.layers.1.fused_bn: torch.Size([128, 96, 96, 96])
features.layers.1.fused_act: torch.Size([128, 96, 96, 96])
features.layers.1.project_conv: torch.Size([128, 48, 96, 96])
features.layers.1.project_bn: torch.Size([128, 48, 96, 96])
features.layers.2.layers.0: torch.Size([128, 48, 96, 96])
features.layers.2.layers.1: torch.Size([128, 48, 96, 96])
features.layers.2.layers.2: torch.Size([128, 192, 96, 96])
features.layers.2.layers.3: torch.Size([128, 192, 96, 96])
features.layers.2.layers.4: torch.Size([128, 192, 96, 96])
features.layers.2.layers.5: torch.Size([128, 192, 48, 48])
features.layers.2.layers.6: torch.Size([128, 192, 48, 48])
features.layers.2.layers.7: torch.Size([128, 192, 48, 48])
features.layers.2.layers.8: torch.Size([128, 96, 48, 48])
features.layers.2.layers.9: torch.Size([128, 96, 48, 48])
features.layers.2.layer_scale: torch.Size([128, 96, 48, 48])
features.layers.3.layers.0: torch.Size([128, 96, 48, 48])
features.layers.3.layers.1: torch.Size([128, 96, 48, 48])
features.layers.3.layers.2: torch.Size([128, 384, 48, 48])
features.layers.3.layers.3: torch.Size([128, 384, 48, 48])
features.layers.3.layers.4: torch.Size([128, 384, 48, 48])
features.layers.3.layers.5: torch.Size([128, 384, 48, 48])
features.layers.3.layers.6: torch.Size([128, 384, 48, 48])
features.layers.3.layers.7: torch.Size([128, 384, 48, 48])
features.layers.3.layers.8: torch.Size([128, 96, 48, 48])
features.layers.3.layers.9: torch.Size([128, 96, 48, 48])
features.layers.3.layer_scale: torch.Size([128, 96, 48, 48])
features.layers.4.layers.0: torch.Size([128, 96, 48, 48])
features.layers.4.layers.1: torch.Size([128, 96, 48, 48])
features.layers.4.layers.2: torch.Size([128, 384, 48, 48])
features.layers.4.layers.3: torch.Size([128, 384, 48, 48])
features.layers.4.layers.4: torch.Size([128, 384, 48, 48])
features.layers.4.layers.5: torch.Size([128, 384, 24, 24])
features.layers.4.layers.6: torch.Size([128, 384, 24, 24])
features.layers.4.layers.7: torch.Size([128, 384, 24, 24])
features.layers.4.layers.8: torch.Size([128, 192, 24, 24])
features.layers.4.layers.9: torch.Size([128, 192, 24, 24])
features.layers.4.layer_scale: torch.Size([128, 192, 24, 24])
features.layers.5.layers.0: torch.Size([128, 192, 24, 24])
features.layers.5.layers.1: torch.Size([128, 192, 24, 24])
features.layers.5.layers.2: torch.Size([128, 768, 24, 24])
features.layers.5.layers.3: torch.Size([128, 768, 24, 24])
features.layers.5.layers.4: torch.Size([128, 768, 24, 24])
features.layers.5.layers.5: torch.Size([128, 768, 24, 24])
features.layers.5.layers.6: torch.Size([128, 768, 24, 24])
features.layers.5.layers.7: torch.Size([128, 768, 24, 24])
features.layers.5.layers.8: torch.Size([128, 192, 24, 24])
features.layers.5.layers.9: torch.Size([128, 192, 24, 24])
features.layers.5.layer_scale: torch.Size([128, 192, 24, 24])
features.layers.6.layers.0: torch.Size([128, 192, 24, 24])
features.layers.6.layers.1: torch.Size([128, 192, 24, 24])
features.layers.6.layers.2: torch.Size([128, 768, 24, 24])
features.layers.6.layers.3: torch.Size([128, 768, 24, 24])
features.layers.6.layers.4: torch.Size([128, 768, 24, 24])
features.layers.6.layers.5: torch.Size([128, 768, 24, 24])
features.layers.6.layers.6: torch.Size([128, 768, 24, 24])
features.layers.6.layers.7: torch.Size([128, 768, 24, 24])
features.layers.6.layers.8: torch.Size([128, 192, 24, 24])
features.layers.6.layers.9: torch.Size([128, 192, 24, 24])
features.layers.6.layer_scale: torch.Size([128, 192, 24, 24])
features.layers.7.layers.0: torch.Size([128, 192, 24, 24])
features.layers.7.layers.1: torch.Size([128, 192, 24, 24])
features.layers.7.layers.2: torch.Size([128, 768, 24, 24])
features.layers.7.layers.3: torch.Size([128, 768, 24, 24])
features.layers.7.layers.4: torch.Size([128, 768, 24, 24])
features.layers.7.layers.5: torch.Size([128, 768, 24, 24])
features.layers.7.layers.6: torch.Size([128, 768, 24, 24])
features.layers.7.layers.7: torch.Size([128, 768, 24, 24])
features.layers.7.layers.8: torch.Size([128, 192, 24, 24])
features.layers.7.layers.9: torch.Size([128, 192, 24, 24])
features.layers.7.layer_scale: torch.Size([128, 192, 24, 24])
features.layers.8.layers.0: torch.Size([128, 192, 24, 24])
features.layers.8.layers.1: torch.Size([128, 192, 24, 24])
features.layers.8.layers.2: torch.Size([128, 768, 24, 24])
features.layers.8.layers.3: torch.Size([128, 768, 24, 24])
features.layers.8.layers.4: torch.Size([128, 768, 24, 24])
features.layers.8.layers.5: torch.Size([128, 768, 24, 24])
features.layers.8.layers.6: torch.Size([128, 768, 24, 24])
features.layers.8.layers.7: torch.Size([128, 768, 24, 24])
features.layers.8.layers.8: torch.Size([128, 192, 24, 24])
features.layers.8.layers.9: torch.Size([128, 192, 24, 24])
features.layers.8.layer_scale: torch.Size([128, 192, 24, 24])
features.layers.9.layers.0: torch.Size([128, 192, 24, 24])
features.layers.9.layers.1: torch.Size([128, 192, 24, 24])
features.layers.9.layers.2: torch.Size([128, 768, 24, 24])
features.layers.9.layers.3: torch.Size([128, 768, 24, 24])
features.layers.9.layers.4: torch.Size([128, 768, 24, 24])
features.layers.9.layers.5: torch.Size([128, 768, 24, 24])
features.layers.9.layers.6: torch.Size([128, 768, 24, 24])
features.layers.9.layers.7: torch.Size([128, 768, 24, 24])
features.layers.9.layers.8: torch.Size([128, 192, 24, 24])
features.layers.9.layers.9: torch.Size([128, 192, 24, 24])
features.layers.9.layer_scale: torch.Size([128, 192, 24, 24])
features.layers.10.layers.0: torch.Size([128, 192, 24, 24])
features.layers.10.layers.1: torch.Size([128, 192, 24, 24])
features.layers.10.layers.2: torch.Size([128, 768, 24, 24])
features.layers.10.layers.3: torch.Size([128, 768, 24, 24])
features.layers.10.layers.4: torch.Size([128, 768, 24, 24])
features.layers.10.layers.5: torch.Size([128, 768, 24, 24])
features.layers.10.layers.6: torch.Size([128, 768, 24, 24])
features.layers.10.layers.7: torch.Size([128, 768, 24, 24])
features.layers.10.layers.8: torch.Size([128, 192, 24, 24])
features.layers.10.layers.9: torch.Size([128, 192, 24, 24])
features.layers.10.layer_scale: torch.Size([128, 192, 24, 24])
features.layers.11.norm: torch.Size([128, 192, 24, 24])
features.layers.11.attention.query_layers.0: torch.Size([128, 192, 24, 24])
features.layers.11.attention.query_layers.1: torch.Size([128, 192, 24, 24])
features.layers.11.attention.query_layers.2: torch.Size([128, 384, 24, 24])
features.layers.11.attention.key_layers.0: torch.Size([128, 384, 12, 12])
features.layers.11.attention.key_layers.1: torch.Size([128, 384, 12, 12])
features.layers.11.attention.key_layers.2: torch.Size([128, 384, 12, 12])
features.layers.11.attention.value_layers.0: torch.Size([128, 384, 6, 6])
features.layers.11.attention.value_layers.1: torch.Size([128, 384, 6, 6])
features.layers.11.attention.value_layers.2: torch.Size([128, 384, 6, 6])
features.layers.11.attention.output_layers.0: torch.Size([128, 384, 6, 6])
features.layers.11.attention.output_layers.1: torch.Size([128, 192, 6, 6])
features.layers.11.attention.dropout_layer: torch.Size([128, 192, 6, 6])
features.layers.11.layer_scale: torch.Size([128, 192, 6, 6])
features.layers.12.layers.0: torch.Size([128, 192, 6, 6])
features.layers.12.layers.1: torch.Size([128, 192, 6, 6])
features.layers.12.layers.2: torch.Size([128, 768, 6, 6])
features.layers.12.layers.3: torch.Size([128, 768, 6, 6])
features.layers.12.layers.4: torch.Size([128, 768, 6, 6])
features.layers.12.layers.5: torch.Size([128, 768, 6, 6])
features.layers.12.layers.6: torch.Size([128, 768, 6, 6])
features.layers.12.layers.7: torch.Size([128, 768, 6, 6])
features.layers.12.layers.8: torch.Size([128, 192, 6, 6])
features.layers.12.layers.9: torch.Size([128, 192, 6, 6])
features.layers.12.layer_scale: torch.Size([128, 192, 6, 6])
features.layers.13.norm: torch.Size([128, 192, 6, 6])
features.layers.13.attention.query_layers.0: torch.Size([128, 192, 6, 6])
features.layers.13.attention.query_layers.1: torch.Size([128, 192, 6, 6])
features.layers.13.attention.query_layers.2: torch.Size([128, 384, 6, 6])
features.layers.13.attention.key_layers.0: torch.Size([128, 384, 3, 3])
features.layers.13.attention.key_layers.1: torch.Size([128, 384, 3, 3])
features.layers.13.attention.key_layers.2: torch.Size([128, 384, 3, 3])
features.layers.13.attention.value_layers.0: torch.Size([128, 384, 2, 2])
features.layers.13.attention.value_layers.1: torch.Size([128, 384, 2, 2])
features.layers.13.attention.value_layers.2: torch.Size([128, 384, 2, 2])
features.layers.13.attention.output_layers.0: torch.Size([128, 384, 2, 2])
features.layers.13.attention.output_layers.1: torch.Size([128, 192, 2, 2])
features.layers.13.attention.dropout_layer: torch.Size([128, 192, 2, 2])
features.layers.13.layer_scale: torch.Size([128, 192, 2, 2])
features.layers.14.layers.0: torch.Size([128, 192, 2, 2])
features.layers.14.layers.1: torch.Size([128, 192, 2, 2])
features.layers.14.layers.2: torch.Size([128, 768, 2, 2])
features.layers.14.layers.3: torch.Size([128, 768, 2, 2])
features.layers.14.layers.4: torch.Size([128, 768, 2, 2])
features.layers.14.layers.5: torch.Size([128, 768, 2, 2])
features.layers.14.layers.6: torch.Size([128, 768, 2, 2])
features.layers.14.layers.7: torch.Size([128, 768, 2, 2])
features.layers.14.layers.8: torch.Size([128, 192, 2, 2])
features.layers.14.layers.9: torch.Size([128, 192, 2, 2])
features.layers.14.layer_scale: torch.Size([128, 192, 2, 2])
features.layers.15.norm: torch.Size([128, 192, 2, 2])
features.layers.15.attention.query_layers.0: torch.Size([128, 192, 2, 2])
features.layers.15.attention.query_layers.1: torch.Size([128, 192, 2, 2])
features.layers.15.attention.query_layers.2: torch.Size([128, 384, 2, 2])
features.layers.15.attention.key_layers.0: torch.Size([128, 384, 1, 1])
features.layers.15.attention.key_layers.1: torch.Size([128, 384, 1, 1])
features.layers.15.attention.key_layers.2: torch.Size([128, 384, 1, 1])
features.layers.15.attention.value_layers.0: torch.Size([128, 384, 1, 1])
features.layers.15.attention.value_layers.1: torch.Size([128, 384, 1, 1])
features.layers.15.attention.value_layers.2: torch.Size([128, 384, 1, 1])
features.layers.15.attention.output_layers.0: torch.Size([128, 384, 1, 1])
features.layers.15.attention.output_layers.1: torch.Size([128, 192, 1, 1])
features.layers.15.attention.dropout_layer: torch.Size([128, 192, 1, 1])
features.layers.15.layer_scale: torch.Size([128, 192, 1, 1])
features.layers.16.layers.0: torch.Size([128, 192, 1, 1])
features.layers.16.layers.1: torch.Size([128, 192, 1, 1])
features.layers.16.layers.2: torch.Size([128, 768, 1, 1])
features.layers.16.layers.3: torch.Size([128, 768, 1, 1])
features.layers.16.layers.4: torch.Size([128, 768, 1, 1])
features.layers.16.layers.5: torch.Size([128, 768, 1, 1])
features.layers.16.layers.6: torch.Size([128, 768, 1, 1])
features.layers.16.layers.7: torch.Size([128, 768, 1, 1])
features.layers.16.layers.8: torch.Size([128, 192, 1, 1])
features.layers.16.layers.9: torch.Size([128, 192, 1, 1])
features.layers.16.layer_scale: torch.Size([128, 192, 1, 1])
features.layers.17.norm: torch.Size([128, 192, 1, 1])
features.layers.17.attention.query_layers.0: torch.Size([128, 192, 1, 1])
features.layers.17.attention.query_layers.1: torch.Size([128, 192, 1, 1])
features.layers.17.attention.query_layers.2: torch.Size([128, 384, 1, 1])
features.layers.17.attention.key_layers.0: torch.Size([128, 384, 1, 1])
features.layers.17.attention.key_layers.1: torch.Size([128, 384, 1, 1])
features.layers.17.attention.key_layers.2: torch.Size([128, 384, 1, 1])
features.layers.17.attention.value_layers.0: torch.Size([128, 384, 1, 1])
features.layers.17.attention.value_layers.1: torch.Size([128, 384, 1, 1])
features.layers.17.attention.value_layers.2: torch.Size([128, 384, 1, 1])
features.layers.17.attention.output_layers.0: torch.Size([128, 384, 1, 1])
features.layers.17.attention.output_layers.1: torch.Size([128, 192, 1, 1])
features.layers.17.attention.dropout_layer: torch.Size([128, 192, 1, 1])
features.layers.17.layer_scale: torch.Size([128, 192, 1, 1])
features.layers.18.layers.0: torch.Size([128, 192, 1, 1])
features.layers.18.layers.1: torch.Size([128, 192, 1, 1])
features.layers.18.layers.2: torch.Size([128, 768, 1, 1])
features.layers.18.layers.3: torch.Size([128, 768, 1, 1])
features.layers.18.layers.4: torch.Size([128, 768, 1, 1])
features.layers.18.layers.5: torch.Size([128, 192, 1, 1])
features.layers.18.layers.6: torch.Size([128, 192, 1, 1])
features.layers.18.layer_scale: torch.Size([128, 192, 1, 1])
features.layers.19.layers.0: torch.Size([128, 192, 1, 1])
features.layers.19.layers.1: torch.Size([128, 192, 1, 1])
features.layers.19.layers.2: torch.Size([128, 768, 1, 1])
features.layers.19.layers.3: torch.Size([128, 768, 1, 1])
features.layers.19.layers.4: torch.Size([128, 768, 1, 1])
features.layers.19.layers.5: torch.Size([128, 768, 1, 1])
features.layers.19.layers.6: torch.Size([128, 768, 1, 1])
features.layers.19.layers.7: torch.Size([128, 768, 1, 1])
features.layers.19.layers.8: torch.Size([128, 512, 1, 1])
features.layers.19.layers.9: torch.Size([128, 512, 1, 1])
features.layers.19.layer_scale: torch.Size([128, 512, 1, 1])
features.layers.20.layers.0: torch.Size([128, 512, 1, 1])
features.layers.20.layers.1: torch.Size([128, 512, 1, 1])
features.layers.20.layers.2: torch.Size([128, 2048, 1, 1])
features.layers.20.layers.3: torch.Size([128, 2048, 1, 1])
features.layers.20.layers.4: torch.Size([128, 2048, 1, 1])
features.layers.20.layers.5: torch.Size([128, 2048, 1, 1])
features.layers.20.layers.6: torch.Size([128, 2048, 1, 1])
features.layers.20.layers.7: torch.Size([128, 2048, 1, 1])
features.layers.20.layers.8: torch.Size([128, 512, 1, 1])
features.layers.20.layers.9: torch.Size([128, 512, 1, 1])
features.layers.20.layer_scale: torch.Size([128, 512, 1, 1])
features.layers.21.layers.0: torch.Size([128, 512, 1, 1])
features.layers.21.layers.1: torch.Size([128, 512, 1, 1])
features.layers.21.layers.2: torch.Size([128, 2048, 1, 1])
features.layers.21.layers.3: torch.Size([128, 2048, 1, 1])
features.layers.21.layers.4: torch.Size([128, 2048, 1, 1])
features.layers.21.layers.5: torch.Size([128, 2048, 1, 1])
features.layers.21.layers.6: torch.Size([128, 2048, 1, 1])
features.layers.21.layers.7: torch.Size([128, 2048, 1, 1])
features.layers.21.layers.8: torch.Size([128, 512, 1, 1])
features.layers.21.layers.9: torch.Size([128, 512, 1, 1])
features.layers.21.layer_scale: torch.Size([128, 512, 1, 1])
features.layers.22.layers.0: torch.Size([128, 512, 1, 1])
features.layers.22.layers.1: torch.Size([128, 512, 1, 1])
features.layers.22.layers.2: torch.Size([128, 2048, 1, 1])
features.layers.22.layers.3: torch.Size([128, 2048, 1, 1])
features.layers.22.layers.4: torch.Size([128, 2048, 1, 1])
features.layers.22.layers.5: torch.Size([128, 2048, 1, 1])
features.layers.22.layers.6: torch.Size([128, 2048, 1, 1])
features.layers.22.layers.7: torch.Size([128, 2048, 1, 1])
features.layers.22.layers.8: torch.Size([128, 512, 1, 1])
features.layers.22.layers.9: torch.Size([128, 512, 1, 1])
features.layers.22.layer_scale: torch.Size([128, 512, 1, 1])
features.layers.23.layers.0: torch.Size([128, 512, 1, 1])
features.layers.23.layers.1: torch.Size([128, 512, 1, 1])
features.layers.23.layers.2: torch.Size([128, 2048, 1, 1])
features.layers.23.layers.3: torch.Size([128, 2048, 1, 1])
features.layers.23.layers.4: torch.Size([128, 2048, 1, 1])
features.layers.23.layers.5: torch.Size([128, 512, 1, 1])
features.layers.23.layers.6: torch.Size([128, 512, 1, 1])
features.layers.23.layer_scale: torch.Size([128, 512, 1, 1])
features.layers.24.layers.0: torch.Size([128, 512, 1, 1])
features.layers.24.layers.1: torch.Size([128, 512, 1, 1])
features.layers.24.layers.2: torch.Size([128, 2048, 1, 1])
features.layers.24.layers.3: torch.Size([128, 2048, 1, 1])
features.layers.24.layers.4: torch.Size([128, 2048, 1, 1])
features.layers.24.layers.5: torch.Size([128, 2048, 1, 1])
features.layers.24.layers.6: torch.Size([128, 2048, 1, 1])
features.layers.24.layers.7: torch.Size([128, 2048, 1, 1])
features.layers.24.layers.8: torch.Size([128, 512, 1, 1])
features.layers.24.layers.9: torch.Size([128, 512, 1, 1])
features.layers.24.layer_scale: torch.Size([128, 512, 1, 1])
features.layers.25.layers.0: torch.Size([128, 512, 1, 1])
features.layers.25.layers.1: torch.Size([128, 512, 1, 1])
features.layers.25.layers.2: torch.Size([128, 2048, 1, 1])
features.layers.25.layers.3: torch.Size([128, 2048, 1, 1])
features.layers.25.layers.4: torch.Size([128, 2048, 1, 1])
features.layers.25.layers.5: torch.Size([128, 512, 1, 1])
features.layers.25.layers.6: torch.Size([128, 512, 1, 1])
features.layers.25.layer_scale: torch.Size([128, 512, 1, 1])
features.layers.26.layers.0: torch.Size([128, 512, 1, 1])
features.layers.26.layers.1: torch.Size([128, 512, 1, 1])
features.layers.26.layers.2: torch.Size([128, 2048, 1, 1])
features.layers.26.layers.3: torch.Size([128, 2048, 1, 1])
features.layers.26.layers.4: torch.Size([128, 2048, 1, 1])
features.layers.26.layers.5: torch.Size([128, 512, 1, 1])
features.layers.26.layers.6: torch.Size([128, 512, 1, 1])
features.layers.26.layer_scale: torch.Size([128, 512, 1, 1])
features.layers.27.layers.0: torch.Size([128, 512, 1, 1])
features.layers.27.layers.1: torch.Size([128, 512, 1, 1])
features.layers.27.layers.2: torch.Size([128, 2048, 1, 1])
features.layers.27.layers.3: torch.Size([128, 2048, 1, 1])
features.layers.27.layers.4: torch.Size([128, 2048, 1, 1])
features.layers.27.layers.5: torch.Size([128, 2048, 1, 1])
features.layers.27.layers.6: torch.Size([128, 2048, 1, 1])
features.layers.27.layers.7: torch.Size([128, 2048, 1, 1])
features.layers.27.layers.8: torch.Size([128, 512, 1, 1])
features.layers.27.layers.9: torch.Size([128, 512, 1, 1])
features.layers.27.layer_scale: torch.Size([128, 512, 1, 1])
features.layers.28.layers.0: torch.Size([128, 512, 1, 1])
features.layers.28.layers.1: torch.Size([128, 512, 1, 1])
features.layers.28.layers.2: torch.Size([128, 2048, 1, 1])
features.layers.28.layers.3: torch.Size([128, 2048, 1, 1])
features.layers.28.layers.4: torch.Size([128, 2048, 1, 1])
features.layers.28.layers.5: torch.Size([128, 2048, 1, 1])
features.layers.28.layers.6: torch.Size([128, 2048, 1, 1])
features.layers.28.layers.7: torch.Size([128, 2048, 1, 1])
features.layers.28.layers.8: torch.Size([128, 512, 1, 1])
features.layers.28.layers.9: torch.Size([128, 512, 1, 1])
features.layers.28.layer_scale: torch.Size([128, 512, 1, 1])
features.layers.29.norm: torch.Size([128, 512, 1, 1])
features.layers.29.attention.query_layers.0: torch.Size([128, 512, 1, 1])
features.layers.29.attention.query_layers.1: torch.Size([128, 512, 1, 1])
features.layers.29.attention.query_layers.2: torch.Size([128, 512, 1, 1])
features.layers.29.attention.key_layers.0: torch.Size([128, 512, 1, 1])
features.layers.29.attention.key_layers.1: torch.Size([128, 512, 1, 1])
features.layers.29.attention.key_layers.2: torch.Size([128, 512, 1, 1])
features.layers.29.attention.value_layers.0: torch.Size([128, 512, 1, 1])
features.layers.29.attention.value_layers.1: torch.Size([128, 512, 1, 1])
features.layers.29.attention.value_layers.2: torch.Size([128, 512, 1, 1])
features.layers.29.attention.output_layers.0: torch.Size([128, 512, 1, 1])
features.layers.29.attention.output_layers.1: torch.Size([128, 512, 1, 1])
features.layers.29.attention.dropout_layer: torch.Size([128, 512, 1, 1])
features.layers.29.layer_scale: torch.Size([128, 512, 1, 1])
features.layers.30.layers.0: torch.Size([128, 512, 1, 1])
features.layers.30.layers.1: torch.Size([128, 512, 1, 1])
features.layers.30.layers.2: torch.Size([128, 2048, 1, 1])
features.layers.30.layers.3: torch.Size([128, 2048, 1, 1])
features.layers.30.layers.4: torch.Size([128, 2048, 1, 1])
features.layers.30.layers.5: torch.Size([128, 512, 1, 1])
features.layers.30.layers.6: torch.Size([128, 512, 1, 1])
features.layers.30.layer_scale: torch.Size([128, 512, 1, 1])
features.layers.31.norm: torch.Size([128, 512, 1, 1])
features.layers.31.attention.query_layers.0: torch.Size([128, 512, 1, 1])
features.layers.31.attention.query_layers.1: torch.Size([128, 512, 1, 1])
features.layers.31.attention.query_layers.2: torch.Size([128, 512, 1, 1])
features.layers.31.attention.key_layers.0: torch.Size([128, 512, 1, 1])
features.layers.31.attention.key_layers.1: torch.Size([128, 512, 1, 1])
features.layers.31.attention.key_layers.2: torch.Size([128, 512, 1, 1])
features.layers.31.attention.value_layers.0: torch.Size([128, 512, 1, 1])
features.layers.31.attention.value_layers.1: torch.Size([128, 512, 1, 1])
features.layers.31.attention.value_layers.2: torch.Size([128, 512, 1, 1])
features.layers.31.attention.output_layers.0: torch.Size([128, 512, 1, 1])
features.layers.31.attention.output_layers.1: torch.Size([128, 512, 1, 1])
features.layers.31.attention.dropout_layer: torch.Size([128, 512, 1, 1])
features.layers.31.layer_scale: torch.Size([128, 512, 1, 1])
features.layers.32.layers.0: torch.Size([128, 512, 1, 1])
features.layers.32.layers.1: torch.Size([128, 512, 1, 1])
features.layers.32.layers.2: torch.Size([128, 2048, 1, 1])
features.layers.32.layers.3: torch.Size([128, 2048, 1, 1])
features.layers.32.layers.4: torch.Size([128, 2048, 1, 1])
features.layers.32.layers.5: torch.Size([128, 512, 1, 1])
features.layers.32.layers.6: torch.Size([128, 512, 1, 1])
features.layers.32.layer_scale: torch.Size([128, 512, 1, 1])
features.layers.33.norm: torch.Size([128, 512, 1, 1])
features.layers.33.attention.query_layers.0: torch.Size([128, 512, 1, 1])
features.layers.33.attention.query_layers.1: torch.Size([128, 512, 1, 1])
features.layers.33.attention.query_layers.2: torch.Size([128, 512, 1, 1])
features.layers.33.attention.key_layers.0: torch.Size([128, 512, 1, 1])
features.layers.33.attention.key_layers.1: torch.Size([128, 512, 1, 1])
features.layers.33.attention.key_layers.2: torch.Size([128, 512, 1, 1])
features.layers.33.attention.value_layers.0: torch.Size([128, 512, 1, 1])
features.layers.33.attention.value_layers.1: torch.Size([128, 512, 1, 1])
features.layers.33.attention.value_layers.2: torch.Size([128, 512, 1, 1])
features.layers.33.attention.output_layers.0: torch.Size([128, 512, 1, 1])
features.layers.33.attention.output_layers.1: torch.Size([128, 512, 1, 1])
features.layers.33.attention.dropout_layer: torch.Size([128, 512, 1, 1])
features.layers.33.layer_scale: torch.Size([128, 512, 1, 1])
features.layers.34.layers.0: torch.Size([128, 512, 1, 1])
features.layers.34.layers.1: torch.Size([128, 512, 1, 1])
features.layers.34.layers.2: torch.Size([128, 2048, 1, 1])
features.layers.34.layers.3: torch.Size([128, 2048, 1, 1])
features.layers.34.layers.4: torch.Size([128, 2048, 1, 1])
features.layers.34.layers.5: torch.Size([128, 512, 1, 1])
features.layers.34.layers.6: torch.Size([128, 512, 1, 1])
features.layers.34.layer_scale: torch.Size([128, 512, 1, 1])
features.layers.35.norm: torch.Size([128, 512, 1, 1])
features.layers.35.attention.query_layers.0: torch.Size([128, 512, 1, 1])
features.layers.35.attention.query_layers.1: torch.Size([128, 512, 1, 1])
features.layers.35.attention.query_layers.2: torch.Size([128, 512, 1, 1])
features.layers.35.attention.key_layers.0: torch.Size([128, 512, 1, 1])
features.layers.35.attention.key_layers.1: torch.Size([128, 512, 1, 1])
features.layers.35.attention.key_layers.2: torch.Size([128, 512, 1, 1])
features.layers.35.attention.value_layers.0: torch.Size([128, 512, 1, 1])
features.layers.35.attention.value_layers.1: torch.Size([128, 512, 1, 1])
features.layers.35.attention.value_layers.2: torch.Size([128, 512, 1, 1])
features.layers.35.attention.output_layers.0: torch.Size([128, 512, 1, 1])
features.layers.35.attention.output_layers.1: torch.Size([128, 512, 1, 1])
features.layers.35.attention.dropout_layer: torch.Size([128, 512, 1, 1])
features.layers.35.layer_scale: torch.Size([128, 512, 1, 1])
features.layers.36.layers.0: torch.Size([128, 512, 1, 1])
features.layers.36.layers.1: torch.Size([128, 512, 1, 1])
features.layers.36.layers.2: torch.Size([128, 2048, 1, 1])
features.layers.36.layers.3: torch.Size([128, 2048, 1, 1])
features.layers.36.layers.4: torch.Size([128, 2048, 1, 1])
features.layers.36.layers.5: torch.Size([128, 512, 1, 1])
features.layers.36.layers.6: torch.Size([128, 512, 1, 1])
features.layers.36.layer_scale: torch.Size([128, 512, 1, 1])
features.layers.37.conv: torch.Size([128, 960, 1, 1])
features.layers.37.bn: torch.Size([128, 960, 1, 1])
features.layers.37.activation_layer: torch.Size([128, 960, 1, 1])
features.layers.38.pool: torch.Size([128, 960, 1, 1])
features.layers.39.conv: torch.Size([128, 1280, 1, 1])
features.layers.39.bn: torch.Size([128, 1280, 1, 1])
features.layers.39.activation_layer: torch.Size([128, 1280, 1, 1])
classifier.0: torch.Size([128, 100, 1, 1])
classifier.1: torch.Size([128, 100])

Final output shape: torch.Size([128, 100])
==================================================

