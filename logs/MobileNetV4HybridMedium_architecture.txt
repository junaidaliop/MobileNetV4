
==================================================
Model: MobileNetV4HybridMedium
==================================================
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1        [128, 32, 127, 127]             864
       BatchNorm2d-2        [128, 32, 127, 127]              64
              ReLU-3        [128, 32, 127, 127]               0
     Conv2DBNBlock-4        [128, 32, 127, 127]               0
            Conv2d-5         [128, 128, 64, 64]          36,864
       BatchNorm2d-6         [128, 128, 64, 64]             256
              ReLU-7         [128, 128, 64, 64]               0
            Conv2d-8          [128, 48, 64, 64]           6,144
       BatchNorm2d-9          [128, 48, 64, 64]              96
FusedInvertedBottleneckBlock-10          [128, 48, 64, 64]               0
           Conv2d-11          [128, 48, 64, 64]             432
      BatchNorm2d-12          [128, 48, 64, 64]              96
           Conv2d-13         [128, 192, 64, 64]           9,216
      BatchNorm2d-14         [128, 192, 64, 64]             384
            ReLU6-15         [128, 192, 64, 64]               0
           Conv2d-16         [128, 192, 32, 32]           4,800
      BatchNorm2d-17         [128, 192, 32, 32]             384
            ReLU6-18         [128, 192, 32, 32]               0
           Conv2d-19          [128, 80, 32, 32]          15,360
      BatchNorm2d-20          [128, 80, 32, 32]             160
   MNV4LayerScale-21          [128, 80, 32, 32]               0
UniversalInvertedBottleneckBlock-22          [128, 80, 32, 32]               0
           Conv2d-23          [128, 80, 32, 32]             720
      BatchNorm2d-24          [128, 80, 32, 32]             160
           Conv2d-25         [128, 160, 32, 32]          12,800
      BatchNorm2d-26         [128, 160, 32, 32]             320
            ReLU6-27         [128, 160, 32, 32]               0
           Conv2d-28         [128, 160, 32, 32]           1,440
      BatchNorm2d-29         [128, 160, 32, 32]             320
            ReLU6-30         [128, 160, 32, 32]               0
           Conv2d-31          [128, 80, 32, 32]          12,800
      BatchNorm2d-32          [128, 80, 32, 32]             160
   MNV4LayerScale-33          [128, 80, 32, 32]               0
UniversalInvertedBottleneckBlock-34          [128, 80, 32, 32]               0
           Conv2d-35          [128, 80, 32, 32]             720
      BatchNorm2d-36          [128, 80, 32, 32]             160
           Conv2d-37         [128, 480, 32, 32]          38,400
      BatchNorm2d-38         [128, 480, 32, 32]             960
            ReLU6-39         [128, 480, 32, 32]               0
           Conv2d-40         [128, 480, 16, 16]          12,000
      BatchNorm2d-41         [128, 480, 16, 16]             960
            ReLU6-42         [128, 480, 16, 16]               0
           Conv2d-43         [128, 160, 16, 16]          76,800
      BatchNorm2d-44         [128, 160, 16, 16]             320
   MNV4LayerScale-45         [128, 160, 16, 16]               0
UniversalInvertedBottleneckBlock-46         [128, 160, 16, 16]               0
           Conv2d-47         [128, 320, 16, 16]          51,200
      BatchNorm2d-48         [128, 320, 16, 16]             640
            ReLU6-49         [128, 320, 16, 16]               0
           Conv2d-50         [128, 160, 16, 16]          51,200
      BatchNorm2d-51         [128, 160, 16, 16]             320
   MNV4LayerScale-52         [128, 160, 16, 16]               0
UniversalInvertedBottleneckBlock-53         [128, 160, 16, 16]               0
           Conv2d-54         [128, 160, 16, 16]           1,440
      BatchNorm2d-55         [128, 160, 16, 16]             320
           Conv2d-56         [128, 640, 16, 16]         102,400
      BatchNorm2d-57         [128, 640, 16, 16]           1,280
            ReLU6-58         [128, 640, 16, 16]               0
           Conv2d-59         [128, 640, 16, 16]           5,760
      BatchNorm2d-60         [128, 640, 16, 16]           1,280
            ReLU6-61         [128, 640, 16, 16]               0
           Conv2d-62         [128, 160, 16, 16]         102,400
      BatchNorm2d-63         [128, 160, 16, 16]             320
   MNV4LayerScale-64         [128, 160, 16, 16]               0
UniversalInvertedBottleneckBlock-65         [128, 160, 16, 16]               0
           Conv2d-66         [128, 160, 16, 16]           1,440
      BatchNorm2d-67         [128, 160, 16, 16]             320
           Conv2d-68         [128, 640, 16, 16]         102,400
      BatchNorm2d-69         [128, 640, 16, 16]           1,280
            ReLU6-70         [128, 640, 16, 16]               0
           Conv2d-71         [128, 640, 16, 16]          16,000
      BatchNorm2d-72         [128, 640, 16, 16]           1,280
            ReLU6-73         [128, 640, 16, 16]               0
           Conv2d-74         [128, 160, 16, 16]         102,400
      BatchNorm2d-75         [128, 160, 16, 16]             320
   MNV4LayerScale-76         [128, 160, 16, 16]               0
UniversalInvertedBottleneckBlock-77         [128, 160, 16, 16]               0
      BatchNorm2d-78         [128, 160, 16, 16]             320
         Identity-79         [128, 160, 16, 16]               0
      BatchNorm2d-80         [128, 160, 16, 16]             320
           Conv2d-81         [128, 256, 16, 16]          40,960
           Conv2d-82           [128, 256, 8, 8]           2,304
      BatchNorm2d-83           [128, 256, 8, 8]             512
           Conv2d-84           [128, 256, 8, 8]          65,536
           Conv2d-85           [128, 256, 8, 8]           2,304
      BatchNorm2d-86           [128, 256, 8, 8]             512
           Conv2d-87           [128, 256, 8, 8]          65,536
          Dropout-88          [128, 4, 256, 64]               0
         Identity-89         [128, 256, 16, 16]               0
           Conv2d-90         [128, 160, 16, 16]          40,960
OptimizedMultiQueryAttentionLayerWithDownSampling-91         [128, 160, 16, 16]               0
   MNV4LayerScale-92         [128, 160, 16, 16]               0
MultiHeadSelfAttentionBlock-93         [128, 160, 16, 16]               0
           Conv2d-94         [128, 160, 16, 16]           1,440
      BatchNorm2d-95         [128, 160, 16, 16]             320
           Conv2d-96         [128, 640, 16, 16]         102,400
      BatchNorm2d-97         [128, 640, 16, 16]           1,280
            ReLU6-98         [128, 640, 16, 16]               0
           Conv2d-99         [128, 640, 16, 16]           5,760
     BatchNorm2d-100         [128, 640, 16, 16]           1,280
           ReLU6-101         [128, 640, 16, 16]               0
          Conv2d-102         [128, 160, 16, 16]         102,400
     BatchNorm2d-103         [128, 160, 16, 16]             320
  MNV4LayerScale-104         [128, 160, 16, 16]               0
UniversalInvertedBottleneckBlock-105         [128, 160, 16, 16]               0
     BatchNorm2d-106         [128, 160, 16, 16]             320
        Identity-107         [128, 160, 16, 16]               0
     BatchNorm2d-108         [128, 160, 16, 16]             320
          Conv2d-109         [128, 256, 16, 16]          40,960
          Conv2d-110           [128, 256, 8, 8]           2,304
     BatchNorm2d-111           [128, 256, 8, 8]             512
          Conv2d-112           [128, 256, 8, 8]          65,536
          Conv2d-113           [128, 256, 8, 8]           2,304
     BatchNorm2d-114           [128, 256, 8, 8]             512
          Conv2d-115           [128, 256, 8, 8]          65,536
         Dropout-116          [128, 4, 256, 64]               0
        Identity-117         [128, 256, 16, 16]               0
          Conv2d-118         [128, 160, 16, 16]          40,960
OptimizedMultiQueryAttentionLayerWithDownSampling-119         [128, 160, 16, 16]               0
  MNV4LayerScale-120         [128, 160, 16, 16]               0
MultiHeadSelfAttentionBlock-121         [128, 160, 16, 16]               0
          Conv2d-122         [128, 160, 16, 16]           1,440
     BatchNorm2d-123         [128, 160, 16, 16]             320
          Conv2d-124         [128, 640, 16, 16]         102,400
     BatchNorm2d-125         [128, 640, 16, 16]           1,280
           ReLU6-126         [128, 640, 16, 16]               0
          Conv2d-127         [128, 160, 16, 16]         102,400
     BatchNorm2d-128         [128, 160, 16, 16]             320
  MNV4LayerScale-129         [128, 160, 16, 16]               0
UniversalInvertedBottleneckBlock-130         [128, 160, 16, 16]               0
     BatchNorm2d-131         [128, 160, 16, 16]             320
        Identity-132         [128, 160, 16, 16]               0
     BatchNorm2d-133         [128, 160, 16, 16]             320
          Conv2d-134         [128, 256, 16, 16]          40,960
          Conv2d-135           [128, 256, 8, 8]           2,304
     BatchNorm2d-136           [128, 256, 8, 8]             512
          Conv2d-137           [128, 256, 8, 8]          65,536
          Conv2d-138           [128, 256, 8, 8]           2,304
     BatchNorm2d-139           [128, 256, 8, 8]             512
          Conv2d-140           [128, 256, 8, 8]          65,536
         Dropout-141          [128, 4, 256, 64]               0
        Identity-142         [128, 256, 16, 16]               0
          Conv2d-143         [128, 160, 16, 16]          40,960
OptimizedMultiQueryAttentionLayerWithDownSampling-144         [128, 160, 16, 16]               0
  MNV4LayerScale-145         [128, 160, 16, 16]               0
MultiHeadSelfAttentionBlock-146         [128, 160, 16, 16]               0
          Conv2d-147         [128, 160, 16, 16]           1,440
     BatchNorm2d-148         [128, 160, 16, 16]             320
          Conv2d-149         [128, 640, 16, 16]         102,400
     BatchNorm2d-150         [128, 640, 16, 16]           1,280
           ReLU6-151         [128, 640, 16, 16]               0
          Conv2d-152         [128, 640, 16, 16]           5,760
     BatchNorm2d-153         [128, 640, 16, 16]           1,280
           ReLU6-154         [128, 640, 16, 16]               0
          Conv2d-155         [128, 160, 16, 16]         102,400
     BatchNorm2d-156         [128, 160, 16, 16]             320
  MNV4LayerScale-157         [128, 160, 16, 16]               0
UniversalInvertedBottleneckBlock-158         [128, 160, 16, 16]               0
     BatchNorm2d-159         [128, 160, 16, 16]             320
        Identity-160         [128, 160, 16, 16]               0
     BatchNorm2d-161         [128, 160, 16, 16]             320
          Conv2d-162         [128, 256, 16, 16]          40,960
          Conv2d-163           [128, 256, 8, 8]           2,304
     BatchNorm2d-164           [128, 256, 8, 8]             512
          Conv2d-165           [128, 256, 8, 8]          65,536
          Conv2d-166           [128, 256, 8, 8]           2,304
     BatchNorm2d-167           [128, 256, 8, 8]             512
          Conv2d-168           [128, 256, 8, 8]          65,536
         Dropout-169          [128, 4, 256, 64]               0
        Identity-170         [128, 256, 16, 16]               0
          Conv2d-171         [128, 160, 16, 16]          40,960
OptimizedMultiQueryAttentionLayerWithDownSampling-172         [128, 160, 16, 16]               0
  MNV4LayerScale-173         [128, 160, 16, 16]               0
MultiHeadSelfAttentionBlock-174         [128, 160, 16, 16]               0
          Conv2d-175         [128, 160, 16, 16]           1,440
     BatchNorm2d-176         [128, 160, 16, 16]             320
          Conv2d-177         [128, 640, 16, 16]         102,400
     BatchNorm2d-178         [128, 640, 16, 16]           1,280
           ReLU6-179         [128, 640, 16, 16]               0
          Conv2d-180         [128, 160, 16, 16]         102,400
     BatchNorm2d-181         [128, 160, 16, 16]             320
  MNV4LayerScale-182         [128, 160, 16, 16]               0
UniversalInvertedBottleneckBlock-183         [128, 160, 16, 16]               0
          Conv2d-184         [128, 160, 16, 16]           4,000
     BatchNorm2d-185         [128, 160, 16, 16]             320
          Conv2d-186         [128, 960, 16, 16]         153,600
     BatchNorm2d-187         [128, 960, 16, 16]           1,920
           ReLU6-188         [128, 960, 16, 16]               0
          Conv2d-189           [128, 960, 8, 8]          24,000
     BatchNorm2d-190           [128, 960, 8, 8]           1,920
           ReLU6-191           [128, 960, 8, 8]               0
          Conv2d-192           [128, 256, 8, 8]         245,760
     BatchNorm2d-193           [128, 256, 8, 8]             512
  MNV4LayerScale-194           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-195           [128, 256, 8, 8]               0
          Conv2d-196           [128, 256, 8, 8]           6,400
     BatchNorm2d-197           [128, 256, 8, 8]             512
          Conv2d-198          [128, 1024, 8, 8]         262,144
     BatchNorm2d-199          [128, 1024, 8, 8]           2,048
           ReLU6-200          [128, 1024, 8, 8]               0
          Conv2d-201          [128, 1024, 8, 8]          25,600
     BatchNorm2d-202          [128, 1024, 8, 8]           2,048
           ReLU6-203          [128, 1024, 8, 8]               0
          Conv2d-204           [128, 256, 8, 8]         262,144
     BatchNorm2d-205           [128, 256, 8, 8]             512
  MNV4LayerScale-206           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-207           [128, 256, 8, 8]               0
          Conv2d-208           [128, 256, 8, 8]           2,304
     BatchNorm2d-209           [128, 256, 8, 8]             512
          Conv2d-210          [128, 1024, 8, 8]         262,144
     BatchNorm2d-211          [128, 1024, 8, 8]           2,048
           ReLU6-212          [128, 1024, 8, 8]               0
          Conv2d-213          [128, 1024, 8, 8]          25,600
     BatchNorm2d-214          [128, 1024, 8, 8]           2,048
           ReLU6-215          [128, 1024, 8, 8]               0
          Conv2d-216           [128, 256, 8, 8]         262,144
     BatchNorm2d-217           [128, 256, 8, 8]             512
  MNV4LayerScale-218           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-219           [128, 256, 8, 8]               0
          Conv2d-220           [128, 256, 8, 8]           2,304
     BatchNorm2d-221           [128, 256, 8, 8]             512
          Conv2d-222          [128, 1024, 8, 8]         262,144
     BatchNorm2d-223          [128, 1024, 8, 8]           2,048
           ReLU6-224          [128, 1024, 8, 8]               0
          Conv2d-225          [128, 1024, 8, 8]          25,600
     BatchNorm2d-226          [128, 1024, 8, 8]           2,048
           ReLU6-227          [128, 1024, 8, 8]               0
          Conv2d-228           [128, 256, 8, 8]         262,144
     BatchNorm2d-229           [128, 256, 8, 8]             512
  MNV4LayerScale-230           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-231           [128, 256, 8, 8]               0
          Conv2d-232           [128, 512, 8, 8]         131,072
     BatchNorm2d-233           [128, 512, 8, 8]           1,024
           ReLU6-234           [128, 512, 8, 8]               0
          Conv2d-235           [128, 256, 8, 8]         131,072
     BatchNorm2d-236           [128, 256, 8, 8]             512
  MNV4LayerScale-237           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-238           [128, 256, 8, 8]               0
          Conv2d-239           [128, 256, 8, 8]           2,304
     BatchNorm2d-240           [128, 256, 8, 8]             512
          Conv2d-241           [128, 512, 8, 8]         131,072
     BatchNorm2d-242           [128, 512, 8, 8]           1,024
           ReLU6-243           [128, 512, 8, 8]               0
          Conv2d-244           [128, 512, 8, 8]          12,800
     BatchNorm2d-245           [128, 512, 8, 8]           1,024
           ReLU6-246           [128, 512, 8, 8]               0
          Conv2d-247           [128, 256, 8, 8]         131,072
     BatchNorm2d-248           [128, 256, 8, 8]             512
  MNV4LayerScale-249           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-250           [128, 256, 8, 8]               0
          Conv2d-251           [128, 512, 8, 8]         131,072
     BatchNorm2d-252           [128, 512, 8, 8]           1,024
           ReLU6-253           [128, 512, 8, 8]               0
          Conv2d-254           [128, 256, 8, 8]         131,072
     BatchNorm2d-255           [128, 256, 8, 8]             512
  MNV4LayerScale-256           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-257           [128, 256, 8, 8]               0
          Conv2d-258          [128, 1024, 8, 8]         262,144
     BatchNorm2d-259          [128, 1024, 8, 8]           2,048
           ReLU6-260          [128, 1024, 8, 8]               0
          Conv2d-261           [128, 256, 8, 8]         262,144
     BatchNorm2d-262           [128, 256, 8, 8]             512
  MNV4LayerScale-263           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-264           [128, 256, 8, 8]               0
     BatchNorm2d-265           [128, 256, 8, 8]             512
        Identity-266           [128, 256, 8, 8]               0
     BatchNorm2d-267           [128, 256, 8, 8]             512
          Conv2d-268           [128, 256, 8, 8]          65,536
        Identity-269           [128, 256, 8, 8]               0
     BatchNorm2d-270           [128, 256, 8, 8]             512
          Conv2d-271           [128, 256, 8, 8]          65,536
        Identity-272           [128, 256, 8, 8]               0
     BatchNorm2d-273           [128, 256, 8, 8]             512
          Conv2d-274           [128, 256, 8, 8]          65,536
         Dropout-275           [128, 4, 64, 64]               0
        Identity-276           [128, 256, 8, 8]               0
          Conv2d-277           [128, 256, 8, 8]          65,536
OptimizedMultiQueryAttentionLayerWithDownSampling-278           [128, 256, 8, 8]               0
  MNV4LayerScale-279           [128, 256, 8, 8]               0
MultiHeadSelfAttentionBlock-280           [128, 256, 8, 8]               0
          Conv2d-281           [128, 256, 8, 8]           2,304
     BatchNorm2d-282           [128, 256, 8, 8]             512
          Conv2d-283          [128, 1024, 8, 8]         262,144
     BatchNorm2d-284          [128, 1024, 8, 8]           2,048
           ReLU6-285          [128, 1024, 8, 8]               0
          Conv2d-286           [128, 256, 8, 8]         262,144
     BatchNorm2d-287           [128, 256, 8, 8]             512
  MNV4LayerScale-288           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-289           [128, 256, 8, 8]               0
     BatchNorm2d-290           [128, 256, 8, 8]             512
        Identity-291           [128, 256, 8, 8]               0
     BatchNorm2d-292           [128, 256, 8, 8]             512
          Conv2d-293           [128, 256, 8, 8]          65,536
        Identity-294           [128, 256, 8, 8]               0
     BatchNorm2d-295           [128, 256, 8, 8]             512
          Conv2d-296           [128, 256, 8, 8]          65,536
        Identity-297           [128, 256, 8, 8]               0
     BatchNorm2d-298           [128, 256, 8, 8]             512
          Conv2d-299           [128, 256, 8, 8]          65,536
         Dropout-300           [128, 4, 64, 64]               0
        Identity-301           [128, 256, 8, 8]               0
          Conv2d-302           [128, 256, 8, 8]          65,536
OptimizedMultiQueryAttentionLayerWithDownSampling-303           [128, 256, 8, 8]               0
  MNV4LayerScale-304           [128, 256, 8, 8]               0
MultiHeadSelfAttentionBlock-305           [128, 256, 8, 8]               0
          Conv2d-306           [128, 256, 8, 8]           6,400
     BatchNorm2d-307           [128, 256, 8, 8]             512
          Conv2d-308          [128, 1024, 8, 8]         262,144
     BatchNorm2d-309          [128, 1024, 8, 8]           2,048
           ReLU6-310          [128, 1024, 8, 8]               0
          Conv2d-311          [128, 1024, 8, 8]          25,600
     BatchNorm2d-312          [128, 1024, 8, 8]           2,048
           ReLU6-313          [128, 1024, 8, 8]               0
          Conv2d-314           [128, 256, 8, 8]         262,144
     BatchNorm2d-315           [128, 256, 8, 8]             512
  MNV4LayerScale-316           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-317           [128, 256, 8, 8]               0
     BatchNorm2d-318           [128, 256, 8, 8]             512
        Identity-319           [128, 256, 8, 8]               0
     BatchNorm2d-320           [128, 256, 8, 8]             512
          Conv2d-321           [128, 256, 8, 8]          65,536
        Identity-322           [128, 256, 8, 8]               0
     BatchNorm2d-323           [128, 256, 8, 8]             512
          Conv2d-324           [128, 256, 8, 8]          65,536
        Identity-325           [128, 256, 8, 8]               0
     BatchNorm2d-326           [128, 256, 8, 8]             512
          Conv2d-327           [128, 256, 8, 8]          65,536
         Dropout-328           [128, 4, 64, 64]               0
        Identity-329           [128, 256, 8, 8]               0
          Conv2d-330           [128, 256, 8, 8]          65,536
OptimizedMultiQueryAttentionLayerWithDownSampling-331           [128, 256, 8, 8]               0
  MNV4LayerScale-332           [128, 256, 8, 8]               0
MultiHeadSelfAttentionBlock-333           [128, 256, 8, 8]               0
          Conv2d-334           [128, 256, 8, 8]           6,400
     BatchNorm2d-335           [128, 256, 8, 8]             512
          Conv2d-336          [128, 1024, 8, 8]         262,144
     BatchNorm2d-337          [128, 1024, 8, 8]           2,048
           ReLU6-338          [128, 1024, 8, 8]               0
          Conv2d-339           [128, 256, 8, 8]         262,144
     BatchNorm2d-340           [128, 256, 8, 8]             512
  MNV4LayerScale-341           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-342           [128, 256, 8, 8]               0
     BatchNorm2d-343           [128, 256, 8, 8]             512
        Identity-344           [128, 256, 8, 8]               0
     BatchNorm2d-345           [128, 256, 8, 8]             512
          Conv2d-346           [128, 256, 8, 8]          65,536
        Identity-347           [128, 256, 8, 8]               0
     BatchNorm2d-348           [128, 256, 8, 8]             512
          Conv2d-349           [128, 256, 8, 8]          65,536
        Identity-350           [128, 256, 8, 8]               0
     BatchNorm2d-351           [128, 256, 8, 8]             512
          Conv2d-352           [128, 256, 8, 8]          65,536
         Dropout-353           [128, 4, 64, 64]               0
        Identity-354           [128, 256, 8, 8]               0
          Conv2d-355           [128, 256, 8, 8]          65,536
OptimizedMultiQueryAttentionLayerWithDownSampling-356           [128, 256, 8, 8]               0
  MNV4LayerScale-357           [128, 256, 8, 8]               0
MultiHeadSelfAttentionBlock-358           [128, 256, 8, 8]               0
          Conv2d-359           [128, 256, 8, 8]           6,400
     BatchNorm2d-360           [128, 256, 8, 8]             512
          Conv2d-361          [128, 1024, 8, 8]         262,144
     BatchNorm2d-362          [128, 1024, 8, 8]           2,048
           ReLU6-363          [128, 1024, 8, 8]               0
          Conv2d-364           [128, 256, 8, 8]         262,144
     BatchNorm2d-365           [128, 256, 8, 8]             512
  MNV4LayerScale-366           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-367           [128, 256, 8, 8]               0
          Conv2d-368           [128, 960, 8, 8]         245,760
     BatchNorm2d-369           [128, 960, 8, 8]           1,920
            ReLU-370           [128, 960, 8, 8]               0
   Conv2DBNBlock-371           [128, 960, 8, 8]               0
AdaptiveAvgPool2d-372           [128, 960, 1, 1]               0
GlobalPoolingBlock-373           [128, 960, 1, 1]               0
          Conv2d-374          [128, 1280, 1, 1]       1,228,800
     BatchNorm2d-375          [128, 1280, 1, 1]           2,560
            ReLU-376          [128, 1280, 1, 1]               0
   Conv2DBNBlock-377          [128, 1280, 1, 1]               0
       MobileNet-378          [128, 1280, 1, 1]               0
          Conv2d-379           [128, 100, 1, 1]         128,100
         Flatten-380                 [128, 100]               0
================================================================
Total params: 10,767,028
Trainable params: 10,767,028
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 96.00
Forward/backward pass size (MB): 26148.45
Params size (MB): 41.07
Estimated Total Size (MB): 26285.52
----------------------------------------------------------------

Detailed layer shapes:
features.layers.0.conv: torch.Size([128, 32, 127, 127])
features.layers.0.bn: torch.Size([128, 32, 127, 127])
features.layers.0.activation_layer: torch.Size([128, 32, 127, 127])
features.layers.1.fused_conv: torch.Size([128, 128, 64, 64])
features.layers.1.fused_bn: torch.Size([128, 128, 64, 64])
features.layers.1.fused_act: torch.Size([128, 128, 64, 64])
features.layers.1.project_conv: torch.Size([128, 48, 64, 64])
features.layers.1.project_bn: torch.Size([128, 48, 64, 64])
features.layers.2.layers.0: torch.Size([128, 48, 64, 64])
features.layers.2.layers.1: torch.Size([128, 48, 64, 64])
features.layers.2.layers.2: torch.Size([128, 192, 64, 64])
features.layers.2.layers.3: torch.Size([128, 192, 64, 64])
features.layers.2.layers.4: torch.Size([128, 192, 64, 64])
features.layers.2.layers.5: torch.Size([128, 192, 32, 32])
features.layers.2.layers.6: torch.Size([128, 192, 32, 32])
features.layers.2.layers.7: torch.Size([128, 192, 32, 32])
features.layers.2.layers.8: torch.Size([128, 80, 32, 32])
features.layers.2.layers.9: torch.Size([128, 80, 32, 32])
features.layers.2.layer_scale: torch.Size([128, 80, 32, 32])
features.layers.3.layers.0: torch.Size([128, 80, 32, 32])
features.layers.3.layers.1: torch.Size([128, 80, 32, 32])
features.layers.3.layers.2: torch.Size([128, 160, 32, 32])
features.layers.3.layers.3: torch.Size([128, 160, 32, 32])
features.layers.3.layers.4: torch.Size([128, 160, 32, 32])
features.layers.3.layers.5: torch.Size([128, 160, 32, 32])
features.layers.3.layers.6: torch.Size([128, 160, 32, 32])
features.layers.3.layers.7: torch.Size([128, 160, 32, 32])
features.layers.3.layers.8: torch.Size([128, 80, 32, 32])
features.layers.3.layers.9: torch.Size([128, 80, 32, 32])
features.layers.3.layer_scale: torch.Size([128, 80, 32, 32])
features.layers.4.layers.0: torch.Size([128, 80, 32, 32])
features.layers.4.layers.1: torch.Size([128, 80, 32, 32])
features.layers.4.layers.2: torch.Size([128, 480, 32, 32])
features.layers.4.layers.3: torch.Size([128, 480, 32, 32])
features.layers.4.layers.4: torch.Size([128, 480, 32, 32])
features.layers.4.layers.5: torch.Size([128, 480, 16, 16])
features.layers.4.layers.6: torch.Size([128, 480, 16, 16])
features.layers.4.layers.7: torch.Size([128, 480, 16, 16])
features.layers.4.layers.8: torch.Size([128, 160, 16, 16])
features.layers.4.layers.9: torch.Size([128, 160, 16, 16])
features.layers.4.layer_scale: torch.Size([128, 160, 16, 16])
features.layers.5.layers.0: torch.Size([128, 320, 16, 16])
features.layers.5.layers.1: torch.Size([128, 320, 16, 16])
features.layers.5.layers.2: torch.Size([128, 320, 16, 16])
features.layers.5.layers.3: torch.Size([128, 160, 16, 16])
features.layers.5.layers.4: torch.Size([128, 160, 16, 16])
features.layers.5.layer_scale: torch.Size([128, 160, 16, 16])
features.layers.6.layers.0: torch.Size([128, 160, 16, 16])
features.layers.6.layers.1: torch.Size([128, 160, 16, 16])
features.layers.6.layers.2: torch.Size([128, 640, 16, 16])
features.layers.6.layers.3: torch.Size([128, 640, 16, 16])
features.layers.6.layers.4: torch.Size([128, 640, 16, 16])
features.layers.6.layers.5: torch.Size([128, 640, 16, 16])
features.layers.6.layers.6: torch.Size([128, 640, 16, 16])
features.layers.6.layers.7: torch.Size([128, 640, 16, 16])
features.layers.6.layers.8: torch.Size([128, 160, 16, 16])
features.layers.6.layers.9: torch.Size([128, 160, 16, 16])
features.layers.6.layer_scale: torch.Size([128, 160, 16, 16])
features.layers.7.layers.0: torch.Size([128, 160, 16, 16])
features.layers.7.layers.1: torch.Size([128, 160, 16, 16])
features.layers.7.layers.2: torch.Size([128, 640, 16, 16])
features.layers.7.layers.3: torch.Size([128, 640, 16, 16])
features.layers.7.layers.4: torch.Size([128, 640, 16, 16])
features.layers.7.layers.5: torch.Size([128, 640, 16, 16])
features.layers.7.layers.6: torch.Size([128, 640, 16, 16])
features.layers.7.layers.7: torch.Size([128, 640, 16, 16])
features.layers.7.layers.8: torch.Size([128, 160, 16, 16])
features.layers.7.layers.9: torch.Size([128, 160, 16, 16])
features.layers.7.layer_scale: torch.Size([128, 160, 16, 16])
features.layers.8.norm: torch.Size([128, 160, 16, 16])
features.layers.8.attention.query_layers.0: torch.Size([128, 160, 16, 16])
features.layers.8.attention.query_layers.1: torch.Size([128, 160, 16, 16])
features.layers.8.attention.query_layers.2: torch.Size([128, 256, 16, 16])
features.layers.8.attention.key_layers.0: torch.Size([128, 256, 8, 8])
features.layers.8.attention.key_layers.1: torch.Size([128, 256, 8, 8])
features.layers.8.attention.key_layers.2: torch.Size([128, 256, 8, 8])
features.layers.8.attention.value_layers.0: torch.Size([128, 256, 4, 4])
features.layers.8.attention.value_layers.1: torch.Size([128, 256, 4, 4])
features.layers.8.attention.value_layers.2: torch.Size([128, 256, 4, 4])
features.layers.8.attention.output_layers.0: torch.Size([128, 256, 4, 4])
features.layers.8.attention.output_layers.1: torch.Size([128, 160, 4, 4])
features.layers.8.attention.dropout_layer: torch.Size([128, 160, 4, 4])
features.layers.8.layer_scale: torch.Size([128, 160, 4, 4])
features.layers.9.layers.0: torch.Size([128, 160, 4, 4])
features.layers.9.layers.1: torch.Size([128, 160, 4, 4])
features.layers.9.layers.2: torch.Size([128, 640, 4, 4])
features.layers.9.layers.3: torch.Size([128, 640, 4, 4])
features.layers.9.layers.4: torch.Size([128, 640, 4, 4])
features.layers.9.layers.5: torch.Size([128, 640, 4, 4])
features.layers.9.layers.6: torch.Size([128, 640, 4, 4])
features.layers.9.layers.7: torch.Size([128, 640, 4, 4])
features.layers.9.layers.8: torch.Size([128, 160, 4, 4])
features.layers.9.layers.9: torch.Size([128, 160, 4, 4])
features.layers.9.layer_scale: torch.Size([128, 160, 4, 4])
features.layers.10.norm: torch.Size([128, 160, 4, 4])
features.layers.10.attention.query_layers.0: torch.Size([128, 160, 4, 4])
features.layers.10.attention.query_layers.1: torch.Size([128, 160, 4, 4])
features.layers.10.attention.query_layers.2: torch.Size([128, 256, 4, 4])
features.layers.10.attention.key_layers.0: torch.Size([128, 256, 2, 2])
features.layers.10.attention.key_layers.1: torch.Size([128, 256, 2, 2])
features.layers.10.attention.key_layers.2: torch.Size([128, 256, 2, 2])
features.layers.10.attention.value_layers.0: torch.Size([128, 256, 1, 1])
features.layers.10.attention.value_layers.1: torch.Size([128, 256, 1, 1])
features.layers.10.attention.value_layers.2: torch.Size([128, 256, 1, 1])
features.layers.10.attention.output_layers.0: torch.Size([128, 256, 1, 1])
features.layers.10.attention.output_layers.1: torch.Size([128, 160, 1, 1])
features.layers.10.attention.dropout_layer: torch.Size([128, 160, 1, 1])
features.layers.10.layer_scale: torch.Size([128, 160, 1, 1])
features.layers.11.layers.0: torch.Size([128, 160, 1, 1])
features.layers.11.layers.1: torch.Size([128, 160, 1, 1])
features.layers.11.layers.2: torch.Size([128, 640, 1, 1])
features.layers.11.layers.3: torch.Size([128, 640, 1, 1])
features.layers.11.layers.4: torch.Size([128, 640, 1, 1])
features.layers.11.layers.5: torch.Size([128, 160, 1, 1])
features.layers.11.layers.6: torch.Size([128, 160, 1, 1])
features.layers.11.layer_scale: torch.Size([128, 160, 1, 1])
features.layers.12.norm: torch.Size([128, 160, 1, 1])
features.layers.12.attention.query_layers.0: torch.Size([128, 160, 1, 1])
features.layers.12.attention.query_layers.1: torch.Size([128, 160, 1, 1])
features.layers.12.attention.query_layers.2: torch.Size([128, 256, 1, 1])
features.layers.12.attention.key_layers.0: torch.Size([128, 256, 1, 1])
features.layers.12.attention.key_layers.1: torch.Size([128, 256, 1, 1])
features.layers.12.attention.key_layers.2: torch.Size([128, 256, 1, 1])
features.layers.12.attention.value_layers.0: torch.Size([128, 256, 1, 1])
features.layers.12.attention.value_layers.1: torch.Size([128, 256, 1, 1])
features.layers.12.attention.value_layers.2: torch.Size([128, 256, 1, 1])
features.layers.12.attention.output_layers.0: torch.Size([128, 256, 1, 1])
features.layers.12.attention.output_layers.1: torch.Size([128, 160, 1, 1])
features.layers.12.attention.dropout_layer: torch.Size([128, 160, 1, 1])
features.layers.12.layer_scale: torch.Size([128, 160, 1, 1])
features.layers.13.layers.0: torch.Size([128, 160, 1, 1])
features.layers.13.layers.1: torch.Size([128, 160, 1, 1])
features.layers.13.layers.2: torch.Size([128, 640, 1, 1])
features.layers.13.layers.3: torch.Size([128, 640, 1, 1])
features.layers.13.layers.4: torch.Size([128, 640, 1, 1])
features.layers.13.layers.5: torch.Size([128, 640, 1, 1])
features.layers.13.layers.6: torch.Size([128, 640, 1, 1])
features.layers.13.layers.7: torch.Size([128, 640, 1, 1])
features.layers.13.layers.8: torch.Size([128, 160, 1, 1])
features.layers.13.layers.9: torch.Size([128, 160, 1, 1])
features.layers.13.layer_scale: torch.Size([128, 160, 1, 1])
features.layers.14.norm: torch.Size([128, 160, 1, 1])
features.layers.14.attention.query_layers.0: torch.Size([128, 160, 1, 1])
features.layers.14.attention.query_layers.1: torch.Size([128, 160, 1, 1])
features.layers.14.attention.query_layers.2: torch.Size([128, 256, 1, 1])
features.layers.14.attention.key_layers.0: torch.Size([128, 256, 1, 1])
features.layers.14.attention.key_layers.1: torch.Size([128, 256, 1, 1])
features.layers.14.attention.key_layers.2: torch.Size([128, 256, 1, 1])
features.layers.14.attention.value_layers.0: torch.Size([128, 256, 1, 1])
features.layers.14.attention.value_layers.1: torch.Size([128, 256, 1, 1])
features.layers.14.attention.value_layers.2: torch.Size([128, 256, 1, 1])
features.layers.14.attention.output_layers.0: torch.Size([128, 256, 1, 1])
features.layers.14.attention.output_layers.1: torch.Size([128, 160, 1, 1])
features.layers.14.attention.dropout_layer: torch.Size([128, 160, 1, 1])
features.layers.14.layer_scale: torch.Size([128, 160, 1, 1])
features.layers.15.layers.0: torch.Size([128, 160, 1, 1])
features.layers.15.layers.1: torch.Size([128, 160, 1, 1])
features.layers.15.layers.2: torch.Size([128, 640, 1, 1])
features.layers.15.layers.3: torch.Size([128, 640, 1, 1])
features.layers.15.layers.4: torch.Size([128, 640, 1, 1])
features.layers.15.layers.5: torch.Size([128, 160, 1, 1])
features.layers.15.layers.6: torch.Size([128, 160, 1, 1])
features.layers.15.layer_scale: torch.Size([128, 160, 1, 1])
features.layers.16.layers.0: torch.Size([128, 160, 1, 1])
features.layers.16.layers.1: torch.Size([128, 160, 1, 1])
features.layers.16.layers.2: torch.Size([128, 960, 1, 1])
features.layers.16.layers.3: torch.Size([128, 960, 1, 1])
features.layers.16.layers.4: torch.Size([128, 960, 1, 1])
features.layers.16.layers.5: torch.Size([128, 960, 1, 1])
features.layers.16.layers.6: torch.Size([128, 960, 1, 1])
features.layers.16.layers.7: torch.Size([128, 960, 1, 1])
features.layers.16.layers.8: torch.Size([128, 256, 1, 1])
features.layers.16.layers.9: torch.Size([128, 256, 1, 1])
features.layers.16.layer_scale: torch.Size([128, 256, 1, 1])
features.layers.17.layers.0: torch.Size([128, 256, 1, 1])
features.layers.17.layers.1: torch.Size([128, 256, 1, 1])
features.layers.17.layers.2: torch.Size([128, 1024, 1, 1])
features.layers.17.layers.3: torch.Size([128, 1024, 1, 1])
features.layers.17.layers.4: torch.Size([128, 1024, 1, 1])
features.layers.17.layers.5: torch.Size([128, 1024, 1, 1])
features.layers.17.layers.6: torch.Size([128, 1024, 1, 1])
features.layers.17.layers.7: torch.Size([128, 1024, 1, 1])
features.layers.17.layers.8: torch.Size([128, 256, 1, 1])
features.layers.17.layers.9: torch.Size([128, 256, 1, 1])
features.layers.17.layer_scale: torch.Size([128, 256, 1, 1])
features.layers.18.layers.0: torch.Size([128, 256, 1, 1])
features.layers.18.layers.1: torch.Size([128, 256, 1, 1])
features.layers.18.layers.2: torch.Size([128, 1024, 1, 1])
features.layers.18.layers.3: torch.Size([128, 1024, 1, 1])
features.layers.18.layers.4: torch.Size([128, 1024, 1, 1])
features.layers.18.layers.5: torch.Size([128, 1024, 1, 1])
features.layers.18.layers.6: torch.Size([128, 1024, 1, 1])
features.layers.18.layers.7: torch.Size([128, 1024, 1, 1])
features.layers.18.layers.8: torch.Size([128, 256, 1, 1])
features.layers.18.layers.9: torch.Size([128, 256, 1, 1])
features.layers.18.layer_scale: torch.Size([128, 256, 1, 1])
features.layers.19.layers.0: torch.Size([128, 256, 1, 1])
features.layers.19.layers.1: torch.Size([128, 256, 1, 1])
features.layers.19.layers.2: torch.Size([128, 1024, 1, 1])
features.layers.19.layers.3: torch.Size([128, 1024, 1, 1])
features.layers.19.layers.4: torch.Size([128, 1024, 1, 1])
features.layers.19.layers.5: torch.Size([128, 1024, 1, 1])
features.layers.19.layers.6: torch.Size([128, 1024, 1, 1])
features.layers.19.layers.7: torch.Size([128, 1024, 1, 1])
features.layers.19.layers.8: torch.Size([128, 256, 1, 1])
features.layers.19.layers.9: torch.Size([128, 256, 1, 1])
features.layers.19.layer_scale: torch.Size([128, 256, 1, 1])
features.layers.20.layers.0: torch.Size([128, 512, 1, 1])
features.layers.20.layers.1: torch.Size([128, 512, 1, 1])
features.layers.20.layers.2: torch.Size([128, 512, 1, 1])
features.layers.20.layers.3: torch.Size([128, 256, 1, 1])
features.layers.20.layers.4: torch.Size([128, 256, 1, 1])
features.layers.20.layer_scale: torch.Size([128, 256, 1, 1])
features.layers.21.layers.0: torch.Size([128, 256, 1, 1])
features.layers.21.layers.1: torch.Size([128, 256, 1, 1])
features.layers.21.layers.2: torch.Size([128, 512, 1, 1])
features.layers.21.layers.3: torch.Size([128, 512, 1, 1])
features.layers.21.layers.4: torch.Size([128, 512, 1, 1])
features.layers.21.layers.5: torch.Size([128, 512, 1, 1])
features.layers.21.layers.6: torch.Size([128, 512, 1, 1])
features.layers.21.layers.7: torch.Size([128, 512, 1, 1])
features.layers.21.layers.8: torch.Size([128, 256, 1, 1])
features.layers.21.layers.9: torch.Size([128, 256, 1, 1])
features.layers.21.layer_scale: torch.Size([128, 256, 1, 1])
features.layers.22.layers.0: torch.Size([128, 512, 1, 1])
features.layers.22.layers.1: torch.Size([128, 512, 1, 1])
features.layers.22.layers.2: torch.Size([128, 512, 1, 1])
features.layers.22.layers.3: torch.Size([128, 256, 1, 1])
features.layers.22.layers.4: torch.Size([128, 256, 1, 1])
features.layers.22.layer_scale: torch.Size([128, 256, 1, 1])
features.layers.23.layers.0: torch.Size([128, 1024, 1, 1])
features.layers.23.layers.1: torch.Size([128, 1024, 1, 1])
features.layers.23.layers.2: torch.Size([128, 1024, 1, 1])
features.layers.23.layers.3: torch.Size([128, 256, 1, 1])
features.layers.23.layers.4: torch.Size([128, 256, 1, 1])
features.layers.23.layer_scale: torch.Size([128, 256, 1, 1])
features.layers.24.norm: torch.Size([128, 256, 1, 1])
features.layers.24.attention.query_layers.0: torch.Size([128, 256, 1, 1])
features.layers.24.attention.query_layers.1: torch.Size([128, 256, 1, 1])
features.layers.24.attention.query_layers.2: torch.Size([128, 256, 1, 1])
features.layers.24.attention.key_layers.0: torch.Size([128, 256, 1, 1])
features.layers.24.attention.key_layers.1: torch.Size([128, 256, 1, 1])
features.layers.24.attention.key_layers.2: torch.Size([128, 256, 1, 1])
features.layers.24.attention.value_layers.0: torch.Size([128, 256, 1, 1])
features.layers.24.attention.value_layers.1: torch.Size([128, 256, 1, 1])
features.layers.24.attention.value_layers.2: torch.Size([128, 256, 1, 1])
features.layers.24.attention.output_layers.0: torch.Size([128, 256, 1, 1])
features.layers.24.attention.output_layers.1: torch.Size([128, 256, 1, 1])
features.layers.24.attention.dropout_layer: torch.Size([128, 256, 1, 1])
features.layers.24.layer_scale: torch.Size([128, 256, 1, 1])
features.layers.25.layers.0: torch.Size([128, 256, 1, 1])
features.layers.25.layers.1: torch.Size([128, 256, 1, 1])
features.layers.25.layers.2: torch.Size([128, 1024, 1, 1])
features.layers.25.layers.3: torch.Size([128, 1024, 1, 1])
features.layers.25.layers.4: torch.Size([128, 1024, 1, 1])
features.layers.25.layers.5: torch.Size([128, 256, 1, 1])
features.layers.25.layers.6: torch.Size([128, 256, 1, 1])
features.layers.25.layer_scale: torch.Size([128, 256, 1, 1])
features.layers.26.norm: torch.Size([128, 256, 1, 1])
features.layers.26.attention.query_layers.0: torch.Size([128, 256, 1, 1])
features.layers.26.attention.query_layers.1: torch.Size([128, 256, 1, 1])
features.layers.26.attention.query_layers.2: torch.Size([128, 256, 1, 1])
features.layers.26.attention.key_layers.0: torch.Size([128, 256, 1, 1])
features.layers.26.attention.key_layers.1: torch.Size([128, 256, 1, 1])
features.layers.26.attention.key_layers.2: torch.Size([128, 256, 1, 1])
features.layers.26.attention.value_layers.0: torch.Size([128, 256, 1, 1])
features.layers.26.attention.value_layers.1: torch.Size([128, 256, 1, 1])
features.layers.26.attention.value_layers.2: torch.Size([128, 256, 1, 1])
features.layers.26.attention.output_layers.0: torch.Size([128, 256, 1, 1])
features.layers.26.attention.output_layers.1: torch.Size([128, 256, 1, 1])
features.layers.26.attention.dropout_layer: torch.Size([128, 256, 1, 1])
features.layers.26.layer_scale: torch.Size([128, 256, 1, 1])
features.layers.27.layers.0: torch.Size([128, 256, 1, 1])
features.layers.27.layers.1: torch.Size([128, 256, 1, 1])
features.layers.27.layers.2: torch.Size([128, 1024, 1, 1])
features.layers.27.layers.3: torch.Size([128, 1024, 1, 1])
features.layers.27.layers.4: torch.Size([128, 1024, 1, 1])
features.layers.27.layers.5: torch.Size([128, 1024, 1, 1])
features.layers.27.layers.6: torch.Size([128, 1024, 1, 1])
features.layers.27.layers.7: torch.Size([128, 1024, 1, 1])
features.layers.27.layers.8: torch.Size([128, 256, 1, 1])
features.layers.27.layers.9: torch.Size([128, 256, 1, 1])
features.layers.27.layer_scale: torch.Size([128, 256, 1, 1])
features.layers.28.norm: torch.Size([128, 256, 1, 1])
features.layers.28.attention.query_layers.0: torch.Size([128, 256, 1, 1])
features.layers.28.attention.query_layers.1: torch.Size([128, 256, 1, 1])
features.layers.28.attention.query_layers.2: torch.Size([128, 256, 1, 1])
features.layers.28.attention.key_layers.0: torch.Size([128, 256, 1, 1])
features.layers.28.attention.key_layers.1: torch.Size([128, 256, 1, 1])
features.layers.28.attention.key_layers.2: torch.Size([128, 256, 1, 1])
features.layers.28.attention.value_layers.0: torch.Size([128, 256, 1, 1])
features.layers.28.attention.value_layers.1: torch.Size([128, 256, 1, 1])
features.layers.28.attention.value_layers.2: torch.Size([128, 256, 1, 1])
features.layers.28.attention.output_layers.0: torch.Size([128, 256, 1, 1])
features.layers.28.attention.output_layers.1: torch.Size([128, 256, 1, 1])
features.layers.28.attention.dropout_layer: torch.Size([128, 256, 1, 1])
features.layers.28.layer_scale: torch.Size([128, 256, 1, 1])
features.layers.29.layers.0: torch.Size([128, 256, 1, 1])
features.layers.29.layers.1: torch.Size([128, 256, 1, 1])
features.layers.29.layers.2: torch.Size([128, 1024, 1, 1])
features.layers.29.layers.3: torch.Size([128, 1024, 1, 1])
features.layers.29.layers.4: torch.Size([128, 1024, 1, 1])
features.layers.29.layers.5: torch.Size([128, 256, 1, 1])
features.layers.29.layers.6: torch.Size([128, 256, 1, 1])
features.layers.29.layer_scale: torch.Size([128, 256, 1, 1])
features.layers.30.norm: torch.Size([128, 256, 1, 1])
features.layers.30.attention.query_layers.0: torch.Size([128, 256, 1, 1])
features.layers.30.attention.query_layers.1: torch.Size([128, 256, 1, 1])
features.layers.30.attention.query_layers.2: torch.Size([128, 256, 1, 1])
features.layers.30.attention.key_layers.0: torch.Size([128, 256, 1, 1])
features.layers.30.attention.key_layers.1: torch.Size([128, 256, 1, 1])
features.layers.30.attention.key_layers.2: torch.Size([128, 256, 1, 1])
features.layers.30.attention.value_layers.0: torch.Size([128, 256, 1, 1])
features.layers.30.attention.value_layers.1: torch.Size([128, 256, 1, 1])
features.layers.30.attention.value_layers.2: torch.Size([128, 256, 1, 1])
features.layers.30.attention.output_layers.0: torch.Size([128, 256, 1, 1])
features.layers.30.attention.output_layers.1: torch.Size([128, 256, 1, 1])
features.layers.30.attention.dropout_layer: torch.Size([128, 256, 1, 1])
features.layers.30.layer_scale: torch.Size([128, 256, 1, 1])
features.layers.31.layers.0: torch.Size([128, 256, 1, 1])
features.layers.31.layers.1: torch.Size([128, 256, 1, 1])
features.layers.31.layers.2: torch.Size([128, 1024, 1, 1])
features.layers.31.layers.3: torch.Size([128, 1024, 1, 1])
features.layers.31.layers.4: torch.Size([128, 1024, 1, 1])
features.layers.31.layers.5: torch.Size([128, 256, 1, 1])
features.layers.31.layers.6: torch.Size([128, 256, 1, 1])
features.layers.31.layer_scale: torch.Size([128, 256, 1, 1])
features.layers.32.conv: torch.Size([128, 960, 1, 1])
features.layers.32.bn: torch.Size([128, 960, 1, 1])
features.layers.32.activation_layer: torch.Size([128, 960, 1, 1])
features.layers.33.pool: torch.Size([128, 960, 1, 1])
features.layers.34.conv: torch.Size([128, 1280, 1, 1])
features.layers.34.bn: torch.Size([128, 1280, 1, 1])
features.layers.34.activation_layer: torch.Size([128, 1280, 1, 1])
classifier.0: torch.Size([128, 100, 1, 1])
classifier.1: torch.Size([128, 100])

Final output shape: torch.Size([128, 100])
==================================================

