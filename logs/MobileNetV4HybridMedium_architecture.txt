
==================================================
Model: MobileNetV4HybridMedium
==================================================
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1        [128, 32, 127, 127]             864
       BatchNorm2d-2        [128, 32, 127, 127]              64
              ReLU-3        [128, 32, 127, 127]               0
     Conv2DBNBlock-4        [128, 32, 127, 127]               0
            Conv2d-5         [128, 128, 64, 64]          36,864
       BatchNorm2d-6         [128, 128, 64, 64]             256
              ReLU-7         [128, 128, 64, 64]               0
            Conv2d-8          [128, 48, 64, 64]           6,144
       BatchNorm2d-9          [128, 48, 64, 64]              96
FusedInvertedBottleneckBlock-10          [128, 48, 64, 64]               0
           Conv2d-11          [128, 48, 64, 64]             432
      BatchNorm2d-12          [128, 48, 64, 64]              96
           Conv2d-13         [128, 192, 64, 64]           9,216
      BatchNorm2d-14         [128, 192, 64, 64]             384
             ReLU-15         [128, 192, 64, 64]               0
           Conv2d-16         [128, 192, 32, 32]           4,800
      BatchNorm2d-17         [128, 192, 32, 32]             384
             ReLU-18         [128, 192, 32, 32]               0
           Conv2d-19          [128, 80, 32, 32]          15,360
      BatchNorm2d-20          [128, 80, 32, 32]             160
   MNV4LayerScale-21          [128, 80, 32, 32]               0
UniversalInvertedBottleneckBlock-22          [128, 80, 32, 32]               0
           Conv2d-23          [128, 80, 32, 32]             720
      BatchNorm2d-24          [128, 80, 32, 32]             160
           Conv2d-25         [128, 160, 32, 32]          12,800
      BatchNorm2d-26         [128, 160, 32, 32]             320
             ReLU-27         [128, 160, 32, 32]               0
           Conv2d-28         [128, 160, 32, 32]           1,440
      BatchNorm2d-29         [128, 160, 32, 32]             320
             ReLU-30         [128, 160, 32, 32]               0
           Conv2d-31          [128, 80, 32, 32]          12,800
      BatchNorm2d-32          [128, 80, 32, 32]             160
   MNV4LayerScale-33          [128, 80, 32, 32]               0
UniversalInvertedBottleneckBlock-34          [128, 80, 32, 32]               0
           Conv2d-35          [128, 80, 32, 32]             720
      BatchNorm2d-36          [128, 80, 32, 32]             160
           Conv2d-37         [128, 480, 32, 32]          38,400
      BatchNorm2d-38         [128, 480, 32, 32]             960
             ReLU-39         [128, 480, 32, 32]               0
           Conv2d-40         [128, 480, 16, 16]          12,000
      BatchNorm2d-41         [128, 480, 16, 16]             960
             ReLU-42         [128, 480, 16, 16]               0
           Conv2d-43         [128, 160, 16, 16]          76,800
      BatchNorm2d-44         [128, 160, 16, 16]             320
   MNV4LayerScale-45         [128, 160, 16, 16]               0
UniversalInvertedBottleneckBlock-46         [128, 160, 16, 16]               0
           Conv2d-47         [128, 320, 16, 16]          51,200
      BatchNorm2d-48         [128, 320, 16, 16]             640
             ReLU-49         [128, 320, 16, 16]               0
           Conv2d-50         [128, 160, 16, 16]          51,200
      BatchNorm2d-51         [128, 160, 16, 16]             320
   MNV4LayerScale-52         [128, 160, 16, 16]               0
UniversalInvertedBottleneckBlock-53         [128, 160, 16, 16]               0
           Conv2d-54         [128, 160, 16, 16]           1,440
      BatchNorm2d-55         [128, 160, 16, 16]             320
           Conv2d-56         [128, 640, 16, 16]         102,400
      BatchNorm2d-57         [128, 640, 16, 16]           1,280
             ReLU-58         [128, 640, 16, 16]               0
           Conv2d-59         [128, 640, 16, 16]           5,760
      BatchNorm2d-60         [128, 640, 16, 16]           1,280
             ReLU-61         [128, 640, 16, 16]               0
           Conv2d-62         [128, 160, 16, 16]         102,400
      BatchNorm2d-63         [128, 160, 16, 16]             320
   MNV4LayerScale-64         [128, 160, 16, 16]               0
UniversalInvertedBottleneckBlock-65         [128, 160, 16, 16]               0
           Conv2d-66         [128, 160, 16, 16]           1,440
      BatchNorm2d-67         [128, 160, 16, 16]             320
           Conv2d-68         [128, 640, 16, 16]         102,400
      BatchNorm2d-69         [128, 640, 16, 16]           1,280
             ReLU-70         [128, 640, 16, 16]               0
           Conv2d-71         [128, 640, 16, 16]          16,000
      BatchNorm2d-72         [128, 640, 16, 16]           1,280
             ReLU-73         [128, 640, 16, 16]               0
           Conv2d-74         [128, 160, 16, 16]         102,400
      BatchNorm2d-75         [128, 160, 16, 16]             320
   MNV4LayerScale-76         [128, 160, 16, 16]               0
UniversalInvertedBottleneckBlock-77         [128, 160, 16, 16]               0
           Conv2d-78         [128, 160, 16, 16]           1,600
      BatchNorm2d-79         [128, 160, 16, 16]             320
           Conv2d-80         [128, 256, 16, 16]          40,960
           Conv2d-81           [128, 160, 8, 8]           1,440
      BatchNorm2d-82           [128, 160, 8, 8]             320
           Conv2d-83            [128, 64, 8, 8]          10,240
          Dropout-84          [128, 256, 4, 64]               0
           Conv2d-85           [128, 160, 8, 8]           1,440
      BatchNorm2d-86           [128, 160, 8, 8]             320
           Conv2d-87            [128, 64, 8, 8]          10,240
           Conv2d-88         [128, 160, 16, 16]          40,960
OptimizedMultiQueryAttentionLayerWithDownSampling-89         [128, 160, 16, 16]               0
   MNV4LayerScale-90         [128, 160, 16, 16]               0
MultiHeadSelfAttentionBlock-91         [128, 160, 16, 16]               0
           Conv2d-92         [128, 160, 16, 16]           1,440
      BatchNorm2d-93         [128, 160, 16, 16]             320
           Conv2d-94         [128, 640, 16, 16]         102,400
      BatchNorm2d-95         [128, 640, 16, 16]           1,280
             ReLU-96         [128, 640, 16, 16]               0
           Conv2d-97         [128, 640, 16, 16]           5,760
      BatchNorm2d-98         [128, 640, 16, 16]           1,280
             ReLU-99         [128, 640, 16, 16]               0
          Conv2d-100         [128, 160, 16, 16]         102,400
     BatchNorm2d-101         [128, 160, 16, 16]             320
  MNV4LayerScale-102         [128, 160, 16, 16]               0
UniversalInvertedBottleneckBlock-103         [128, 160, 16, 16]               0
          Conv2d-104         [128, 160, 16, 16]           1,600
     BatchNorm2d-105         [128, 160, 16, 16]             320
          Conv2d-106         [128, 256, 16, 16]          40,960
          Conv2d-107           [128, 160, 8, 8]           1,440
     BatchNorm2d-108           [128, 160, 8, 8]             320
          Conv2d-109            [128, 64, 8, 8]          10,240
         Dropout-110          [128, 256, 4, 64]               0
          Conv2d-111           [128, 160, 8, 8]           1,440
     BatchNorm2d-112           [128, 160, 8, 8]             320
          Conv2d-113            [128, 64, 8, 8]          10,240
          Conv2d-114         [128, 160, 16, 16]          40,960
OptimizedMultiQueryAttentionLayerWithDownSampling-115         [128, 160, 16, 16]               0
  MNV4LayerScale-116         [128, 160, 16, 16]               0
MultiHeadSelfAttentionBlock-117         [128, 160, 16, 16]               0
          Conv2d-118         [128, 160, 16, 16]           1,440
     BatchNorm2d-119         [128, 160, 16, 16]             320
          Conv2d-120         [128, 640, 16, 16]         102,400
     BatchNorm2d-121         [128, 640, 16, 16]           1,280
            ReLU-122         [128, 640, 16, 16]               0
          Conv2d-123         [128, 160, 16, 16]         102,400
     BatchNorm2d-124         [128, 160, 16, 16]             320
  MNV4LayerScale-125         [128, 160, 16, 16]               0
UniversalInvertedBottleneckBlock-126         [128, 160, 16, 16]               0
          Conv2d-127         [128, 160, 16, 16]           1,600
     BatchNorm2d-128         [128, 160, 16, 16]             320
          Conv2d-129         [128, 256, 16, 16]          40,960
          Conv2d-130           [128, 160, 8, 8]           1,440
     BatchNorm2d-131           [128, 160, 8, 8]             320
          Conv2d-132            [128, 64, 8, 8]          10,240
         Dropout-133          [128, 256, 4, 64]               0
          Conv2d-134           [128, 160, 8, 8]           1,440
     BatchNorm2d-135           [128, 160, 8, 8]             320
          Conv2d-136            [128, 64, 8, 8]          10,240
          Conv2d-137         [128, 160, 16, 16]          40,960
OptimizedMultiQueryAttentionLayerWithDownSampling-138         [128, 160, 16, 16]               0
  MNV4LayerScale-139         [128, 160, 16, 16]               0
MultiHeadSelfAttentionBlock-140         [128, 160, 16, 16]               0
          Conv2d-141         [128, 160, 16, 16]           1,440
     BatchNorm2d-142         [128, 160, 16, 16]             320
          Conv2d-143         [128, 640, 16, 16]         102,400
     BatchNorm2d-144         [128, 640, 16, 16]           1,280
            ReLU-145         [128, 640, 16, 16]               0
          Conv2d-146         [128, 640, 16, 16]           5,760
     BatchNorm2d-147         [128, 640, 16, 16]           1,280
            ReLU-148         [128, 640, 16, 16]               0
          Conv2d-149         [128, 160, 16, 16]         102,400
     BatchNorm2d-150         [128, 160, 16, 16]             320
  MNV4LayerScale-151         [128, 160, 16, 16]               0
UniversalInvertedBottleneckBlock-152         [128, 160, 16, 16]               0
          Conv2d-153         [128, 160, 16, 16]           1,600
     BatchNorm2d-154         [128, 160, 16, 16]             320
          Conv2d-155         [128, 256, 16, 16]          40,960
          Conv2d-156           [128, 160, 8, 8]           1,440
     BatchNorm2d-157           [128, 160, 8, 8]             320
          Conv2d-158            [128, 64, 8, 8]          10,240
         Dropout-159          [128, 256, 4, 64]               0
          Conv2d-160           [128, 160, 8, 8]           1,440
     BatchNorm2d-161           [128, 160, 8, 8]             320
          Conv2d-162            [128, 64, 8, 8]          10,240
          Conv2d-163         [128, 160, 16, 16]          40,960
OptimizedMultiQueryAttentionLayerWithDownSampling-164         [128, 160, 16, 16]               0
  MNV4LayerScale-165         [128, 160, 16, 16]               0
MultiHeadSelfAttentionBlock-166         [128, 160, 16, 16]               0
          Conv2d-167         [128, 160, 16, 16]           1,440
     BatchNorm2d-168         [128, 160, 16, 16]             320
          Conv2d-169         [128, 640, 16, 16]         102,400
     BatchNorm2d-170         [128, 640, 16, 16]           1,280
            ReLU-171         [128, 640, 16, 16]               0
          Conv2d-172         [128, 160, 16, 16]         102,400
     BatchNorm2d-173         [128, 160, 16, 16]             320
  MNV4LayerScale-174         [128, 160, 16, 16]               0
UniversalInvertedBottleneckBlock-175         [128, 160, 16, 16]               0
          Conv2d-176         [128, 160, 16, 16]           4,000
     BatchNorm2d-177         [128, 160, 16, 16]             320
          Conv2d-178         [128, 960, 16, 16]         153,600
     BatchNorm2d-179         [128, 960, 16, 16]           1,920
            ReLU-180         [128, 960, 16, 16]               0
          Conv2d-181           [128, 960, 8, 8]          24,000
     BatchNorm2d-182           [128, 960, 8, 8]           1,920
            ReLU-183           [128, 960, 8, 8]               0
          Conv2d-184           [128, 256, 8, 8]         245,760
     BatchNorm2d-185           [128, 256, 8, 8]             512
  MNV4LayerScale-186           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-187           [128, 256, 8, 8]               0
          Conv2d-188           [128, 256, 8, 8]           6,400
     BatchNorm2d-189           [128, 256, 8, 8]             512
          Conv2d-190          [128, 1024, 8, 8]         262,144
     BatchNorm2d-191          [128, 1024, 8, 8]           2,048
            ReLU-192          [128, 1024, 8, 8]               0
          Conv2d-193          [128, 1024, 8, 8]          25,600
     BatchNorm2d-194          [128, 1024, 8, 8]           2,048
            ReLU-195          [128, 1024, 8, 8]               0
          Conv2d-196           [128, 256, 8, 8]         262,144
     BatchNorm2d-197           [128, 256, 8, 8]             512
  MNV4LayerScale-198           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-199           [128, 256, 8, 8]               0
          Conv2d-200           [128, 256, 8, 8]           2,304
     BatchNorm2d-201           [128, 256, 8, 8]             512
          Conv2d-202          [128, 1024, 8, 8]         262,144
     BatchNorm2d-203          [128, 1024, 8, 8]           2,048
            ReLU-204          [128, 1024, 8, 8]               0
          Conv2d-205          [128, 1024, 8, 8]          25,600
     BatchNorm2d-206          [128, 1024, 8, 8]           2,048
            ReLU-207          [128, 1024, 8, 8]               0
          Conv2d-208           [128, 256, 8, 8]         262,144
     BatchNorm2d-209           [128, 256, 8, 8]             512
  MNV4LayerScale-210           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-211           [128, 256, 8, 8]               0
          Conv2d-212           [128, 256, 8, 8]           2,304
     BatchNorm2d-213           [128, 256, 8, 8]             512
          Conv2d-214          [128, 1024, 8, 8]         262,144
     BatchNorm2d-215          [128, 1024, 8, 8]           2,048
            ReLU-216          [128, 1024, 8, 8]               0
          Conv2d-217          [128, 1024, 8, 8]          25,600
     BatchNorm2d-218          [128, 1024, 8, 8]           2,048
            ReLU-219          [128, 1024, 8, 8]               0
          Conv2d-220           [128, 256, 8, 8]         262,144
     BatchNorm2d-221           [128, 256, 8, 8]             512
  MNV4LayerScale-222           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-223           [128, 256, 8, 8]               0
          Conv2d-224           [128, 512, 8, 8]         131,072
     BatchNorm2d-225           [128, 512, 8, 8]           1,024
            ReLU-226           [128, 512, 8, 8]               0
          Conv2d-227           [128, 256, 8, 8]         131,072
     BatchNorm2d-228           [128, 256, 8, 8]             512
  MNV4LayerScale-229           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-230           [128, 256, 8, 8]               0
          Conv2d-231           [128, 256, 8, 8]           2,304
     BatchNorm2d-232           [128, 256, 8, 8]             512
          Conv2d-233           [128, 512, 8, 8]         131,072
     BatchNorm2d-234           [128, 512, 8, 8]           1,024
            ReLU-235           [128, 512, 8, 8]               0
          Conv2d-236           [128, 512, 8, 8]          12,800
     BatchNorm2d-237           [128, 512, 8, 8]           1,024
            ReLU-238           [128, 512, 8, 8]               0
          Conv2d-239           [128, 256, 8, 8]         131,072
     BatchNorm2d-240           [128, 256, 8, 8]             512
  MNV4LayerScale-241           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-242           [128, 256, 8, 8]               0
          Conv2d-243           [128, 512, 8, 8]         131,072
     BatchNorm2d-244           [128, 512, 8, 8]           1,024
            ReLU-245           [128, 512, 8, 8]               0
          Conv2d-246           [128, 256, 8, 8]         131,072
     BatchNorm2d-247           [128, 256, 8, 8]             512
  MNV4LayerScale-248           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-249           [128, 256, 8, 8]               0
          Conv2d-250          [128, 1024, 8, 8]         262,144
     BatchNorm2d-251          [128, 1024, 8, 8]           2,048
            ReLU-252          [128, 1024, 8, 8]               0
          Conv2d-253           [128, 256, 8, 8]         262,144
     BatchNorm2d-254           [128, 256, 8, 8]             512
  MNV4LayerScale-255           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-256           [128, 256, 8, 8]               0
          Conv2d-257           [128, 256, 8, 8]           2,560
     BatchNorm2d-258           [128, 256, 8, 8]             512
  MNV4LayerScale-259           [128, 256, 8, 8]               0
MultiHeadSelfAttentionBlock-260           [128, 256, 8, 8]               0
          Conv2d-261           [128, 256, 8, 8]           2,304
     BatchNorm2d-262           [128, 256, 8, 8]             512
          Conv2d-263          [128, 1024, 8, 8]         262,144
     BatchNorm2d-264          [128, 1024, 8, 8]           2,048
            ReLU-265          [128, 1024, 8, 8]               0
          Conv2d-266           [128, 256, 8, 8]         262,144
     BatchNorm2d-267           [128, 256, 8, 8]             512
  MNV4LayerScale-268           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-269           [128, 256, 8, 8]               0
          Conv2d-270           [128, 256, 8, 8]           2,560
     BatchNorm2d-271           [128, 256, 8, 8]             512
  MNV4LayerScale-272           [128, 256, 8, 8]               0
MultiHeadSelfAttentionBlock-273           [128, 256, 8, 8]               0
          Conv2d-274           [128, 256, 8, 8]           6,400
     BatchNorm2d-275           [128, 256, 8, 8]             512
          Conv2d-276          [128, 1024, 8, 8]         262,144
     BatchNorm2d-277          [128, 1024, 8, 8]           2,048
            ReLU-278          [128, 1024, 8, 8]               0
          Conv2d-279          [128, 1024, 8, 8]          25,600
     BatchNorm2d-280          [128, 1024, 8, 8]           2,048
            ReLU-281          [128, 1024, 8, 8]               0
          Conv2d-282           [128, 256, 8, 8]         262,144
     BatchNorm2d-283           [128, 256, 8, 8]             512
  MNV4LayerScale-284           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-285           [128, 256, 8, 8]               0
          Conv2d-286           [128, 256, 8, 8]           2,560
     BatchNorm2d-287           [128, 256, 8, 8]             512
  MNV4LayerScale-288           [128, 256, 8, 8]               0
MultiHeadSelfAttentionBlock-289           [128, 256, 8, 8]               0
          Conv2d-290           [128, 256, 8, 8]           6,400
     BatchNorm2d-291           [128, 256, 8, 8]             512
          Conv2d-292          [128, 1024, 8, 8]         262,144
     BatchNorm2d-293          [128, 1024, 8, 8]           2,048
            ReLU-294          [128, 1024, 8, 8]               0
          Conv2d-295           [128, 256, 8, 8]         262,144
     BatchNorm2d-296           [128, 256, 8, 8]             512
  MNV4LayerScale-297           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-298           [128, 256, 8, 8]               0
          Conv2d-299           [128, 256, 8, 8]           2,560
     BatchNorm2d-300           [128, 256, 8, 8]             512
  MNV4LayerScale-301           [128, 256, 8, 8]               0
MultiHeadSelfAttentionBlock-302           [128, 256, 8, 8]               0
          Conv2d-303           [128, 256, 8, 8]           6,400
     BatchNorm2d-304           [128, 256, 8, 8]             512
          Conv2d-305          [128, 1024, 8, 8]         262,144
     BatchNorm2d-306          [128, 1024, 8, 8]           2,048
            ReLU-307          [128, 1024, 8, 8]               0
          Conv2d-308           [128, 256, 8, 8]         262,144
     BatchNorm2d-309           [128, 256, 8, 8]             512
  MNV4LayerScale-310           [128, 256, 8, 8]               0
UniversalInvertedBottleneckBlock-311           [128, 256, 8, 8]               0
          Conv2d-312           [128, 960, 8, 8]         245,760
     BatchNorm2d-313           [128, 960, 8, 8]           1,920
            ReLU-314           [128, 960, 8, 8]               0
   Conv2DBNBlock-315           [128, 960, 8, 8]               0
AdaptiveAvgPool2d-316           [128, 960, 1, 1]               0
GlobalPoolingBlock-317           [128, 960, 1, 1]               0
          Conv2d-318          [128, 1280, 1, 1]       1,228,800
     BatchNorm2d-319          [128, 1280, 1, 1]           2,560
            ReLU-320          [128, 1280, 1, 1]               0
   Conv2DBNBlock-321          [128, 1280, 1, 1]               0
       MobileNet-322          [128, 1280, 1, 1]               0
          Conv2d-323          [128, 1000, 1, 1]       1,281,000
         Flatten-324                [128, 1000]               0
================================================================
Total params: 10,429,752
Trainable params: 10,429,752
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 96.00
Forward/backward pass size (MB): 24774.20
Params size (MB): 39.79
Estimated Total Size (MB): 24909.99
----------------------------------------------------------------

Detailed layer shapes:
features.layers.0.conv: torch.Size([128, 32, 127, 127])
features.layers.0.bn: torch.Size([128, 32, 127, 127])
features.layers.0.activation_layer: torch.Size([128, 32, 127, 127])
features.layers.1.fused_conv: torch.Size([128, 128, 64, 64])
features.layers.1.fused_bn: torch.Size([128, 128, 64, 64])
features.layers.1.fused_act: torch.Size([128, 128, 64, 64])
features.layers.1.project_conv: torch.Size([128, 48, 64, 64])
features.layers.1.project_bn: torch.Size([128, 48, 64, 64])
features.layers.2.layers.0: torch.Size([128, 48, 64, 64])
features.layers.2.layers.1: torch.Size([128, 48, 64, 64])
features.layers.2.layers.2: torch.Size([128, 192, 64, 64])
features.layers.2.layers.3: torch.Size([128, 192, 64, 64])
features.layers.2.layers.4: torch.Size([128, 192, 64, 64])
features.layers.2.layers.5: torch.Size([128, 192, 32, 32])
features.layers.2.layers.6: torch.Size([128, 192, 32, 32])
features.layers.2.layers.7: torch.Size([128, 192, 32, 32])
features.layers.2.layers.8: torch.Size([128, 80, 32, 32])
features.layers.2.layers.9: torch.Size([128, 80, 32, 32])
features.layers.2.layer_scale: torch.Size([128, 80, 32, 32])
features.layers.3.layers.0: torch.Size([128, 80, 32, 32])
features.layers.3.layers.1: torch.Size([128, 80, 32, 32])
features.layers.3.layers.2: torch.Size([128, 160, 32, 32])
features.layers.3.layers.3: torch.Size([128, 160, 32, 32])
features.layers.3.layers.4: torch.Size([128, 160, 32, 32])
features.layers.3.layers.5: torch.Size([128, 160, 32, 32])
features.layers.3.layers.6: torch.Size([128, 160, 32, 32])
features.layers.3.layers.7: torch.Size([128, 160, 32, 32])
features.layers.3.layers.8: torch.Size([128, 80, 32, 32])
features.layers.3.layers.9: torch.Size([128, 80, 32, 32])
features.layers.3.layer_scale: torch.Size([128, 80, 32, 32])
features.layers.4.layers.0: torch.Size([128, 80, 32, 32])
features.layers.4.layers.1: torch.Size([128, 80, 32, 32])
features.layers.4.layers.2: torch.Size([128, 480, 32, 32])
features.layers.4.layers.3: torch.Size([128, 480, 32, 32])
features.layers.4.layers.4: torch.Size([128, 480, 32, 32])
features.layers.4.layers.5: torch.Size([128, 480, 16, 16])
features.layers.4.layers.6: torch.Size([128, 480, 16, 16])
features.layers.4.layers.7: torch.Size([128, 480, 16, 16])
features.layers.4.layers.8: torch.Size([128, 160, 16, 16])
features.layers.4.layers.9: torch.Size([128, 160, 16, 16])
features.layers.4.layer_scale: torch.Size([128, 160, 16, 16])
features.layers.5.layers.0: torch.Size([128, 320, 16, 16])
features.layers.5.layers.1: torch.Size([128, 320, 16, 16])
features.layers.5.layers.2: torch.Size([128, 320, 16, 16])
features.layers.5.layers.3: torch.Size([128, 160, 16, 16])
features.layers.5.layers.4: torch.Size([128, 160, 16, 16])
features.layers.5.layer_scale: torch.Size([128, 160, 16, 16])
features.layers.6.layers.0: torch.Size([128, 160, 16, 16])
features.layers.6.layers.1: torch.Size([128, 160, 16, 16])
features.layers.6.layers.2: torch.Size([128, 640, 16, 16])
features.layers.6.layers.3: torch.Size([128, 640, 16, 16])
features.layers.6.layers.4: torch.Size([128, 640, 16, 16])
features.layers.6.layers.5: torch.Size([128, 640, 16, 16])
features.layers.6.layers.6: torch.Size([128, 640, 16, 16])
features.layers.6.layers.7: torch.Size([128, 640, 16, 16])
features.layers.6.layers.8: torch.Size([128, 160, 16, 16])
features.layers.6.layers.9: torch.Size([128, 160, 16, 16])
features.layers.6.layer_scale: torch.Size([128, 160, 16, 16])
features.layers.7.layers.0: torch.Size([128, 160, 16, 16])
features.layers.7.layers.1: torch.Size([128, 160, 16, 16])
features.layers.7.layers.2: torch.Size([128, 640, 16, 16])
features.layers.7.layers.3: torch.Size([128, 640, 16, 16])
features.layers.7.layers.4: torch.Size([128, 640, 16, 16])
features.layers.7.layers.5: torch.Size([128, 640, 16, 16])
features.layers.7.layers.6: torch.Size([128, 640, 16, 16])
features.layers.7.layers.7: torch.Size([128, 640, 16, 16])
features.layers.7.layers.8: torch.Size([128, 160, 16, 16])
features.layers.7.layers.9: torch.Size([128, 160, 16, 16])
features.layers.7.layer_scale: torch.Size([128, 160, 16, 16])
features.layers.8.input_norm: torch.Size([128, 160, 16, 16])
features.layers.8.cpe_dw_conv: torch.Size([128, 160, 16, 16])
features.layers.8.multi_query_attention.query_proj: torch.Size([128, 256, 16, 16])
Error in layer features.layers.8.multi_query_attention.key_dw_conv: Given groups=160, weight of size [160, 1, 3, 3], expected input[128, 256, 16, 16] to have 160 channels, but got 256 channels instead
Error in layer features.layers.8.multi_query_attention.key_dw_norm: running_mean should contain 256 elements not 160
Error in layer features.layers.8.multi_query_attention.key_proj: Given groups=1, weight of size [64, 160, 1, 1], expected input[128, 256, 16, 16] to have 160 channels, but got 256 channels instead
Error in layer features.layers.8.multi_query_attention.value_dw_conv: Given groups=160, weight of size [160, 1, 3, 3], expected input[128, 256, 16, 16] to have 160 channels, but got 256 channels instead
Error in layer features.layers.8.multi_query_attention.value_dw_norm: running_mean should contain 256 elements not 160
Error in layer features.layers.8.multi_query_attention.value_proj: Given groups=1, weight of size [64, 160, 1, 1], expected input[128, 256, 16, 16] to have 160 channels, but got 256 channels instead
features.layers.8.multi_query_attention.output_proj: torch.Size([128, 160, 16, 16])
features.layers.8.multi_query_attention.dropout_layer: torch.Size([128, 160, 16, 16])
features.layers.8.layer_scale: torch.Size([128, 160, 16, 16])
features.layers.9.layers.0: torch.Size([128, 160, 16, 16])
features.layers.9.layers.1: torch.Size([128, 160, 16, 16])
features.layers.9.layers.2: torch.Size([128, 640, 16, 16])
features.layers.9.layers.3: torch.Size([128, 640, 16, 16])
features.layers.9.layers.4: torch.Size([128, 640, 16, 16])
features.layers.9.layers.5: torch.Size([128, 640, 16, 16])
features.layers.9.layers.6: torch.Size([128, 640, 16, 16])
features.layers.9.layers.7: torch.Size([128, 640, 16, 16])
features.layers.9.layers.8: torch.Size([128, 160, 16, 16])
features.layers.9.layers.9: torch.Size([128, 160, 16, 16])
features.layers.9.layer_scale: torch.Size([128, 160, 16, 16])
features.layers.10.input_norm: torch.Size([128, 160, 16, 16])
features.layers.10.cpe_dw_conv: torch.Size([128, 160, 16, 16])
features.layers.10.multi_query_attention.query_proj: torch.Size([128, 256, 16, 16])
Error in layer features.layers.10.multi_query_attention.key_dw_conv: Given groups=160, weight of size [160, 1, 3, 3], expected input[128, 256, 16, 16] to have 160 channels, but got 256 channels instead
Error in layer features.layers.10.multi_query_attention.key_dw_norm: running_mean should contain 256 elements not 160
Error in layer features.layers.10.multi_query_attention.key_proj: Given groups=1, weight of size [64, 160, 1, 1], expected input[128, 256, 16, 16] to have 160 channels, but got 256 channels instead
Error in layer features.layers.10.multi_query_attention.value_dw_conv: Given groups=160, weight of size [160, 1, 3, 3], expected input[128, 256, 16, 16] to have 160 channels, but got 256 channels instead
Error in layer features.layers.10.multi_query_attention.value_dw_norm: running_mean should contain 256 elements not 160
Error in layer features.layers.10.multi_query_attention.value_proj: Given groups=1, weight of size [64, 160, 1, 1], expected input[128, 256, 16, 16] to have 160 channels, but got 256 channels instead
features.layers.10.multi_query_attention.output_proj: torch.Size([128, 160, 16, 16])
features.layers.10.multi_query_attention.dropout_layer: torch.Size([128, 160, 16, 16])
features.layers.10.layer_scale: torch.Size([128, 160, 16, 16])
features.layers.11.layers.0: torch.Size([128, 160, 16, 16])
features.layers.11.layers.1: torch.Size([128, 160, 16, 16])
features.layers.11.layers.2: torch.Size([128, 640, 16, 16])
features.layers.11.layers.3: torch.Size([128, 640, 16, 16])
features.layers.11.layers.4: torch.Size([128, 640, 16, 16])
features.layers.11.layers.5: torch.Size([128, 160, 16, 16])
features.layers.11.layers.6: torch.Size([128, 160, 16, 16])
features.layers.11.layer_scale: torch.Size([128, 160, 16, 16])
features.layers.12.input_norm: torch.Size([128, 160, 16, 16])
features.layers.12.cpe_dw_conv: torch.Size([128, 160, 16, 16])
features.layers.12.multi_query_attention.query_proj: torch.Size([128, 256, 16, 16])
Error in layer features.layers.12.multi_query_attention.key_dw_conv: Given groups=160, weight of size [160, 1, 3, 3], expected input[128, 256, 16, 16] to have 160 channels, but got 256 channels instead
Error in layer features.layers.12.multi_query_attention.key_dw_norm: running_mean should contain 256 elements not 160
Error in layer features.layers.12.multi_query_attention.key_proj: Given groups=1, weight of size [64, 160, 1, 1], expected input[128, 256, 16, 16] to have 160 channels, but got 256 channels instead
Error in layer features.layers.12.multi_query_attention.value_dw_conv: Given groups=160, weight of size [160, 1, 3, 3], expected input[128, 256, 16, 16] to have 160 channels, but got 256 channels instead
Error in layer features.layers.12.multi_query_attention.value_dw_norm: running_mean should contain 256 elements not 160
Error in layer features.layers.12.multi_query_attention.value_proj: Given groups=1, weight of size [64, 160, 1, 1], expected input[128, 256, 16, 16] to have 160 channels, but got 256 channels instead
features.layers.12.multi_query_attention.output_proj: torch.Size([128, 160, 16, 16])
features.layers.12.multi_query_attention.dropout_layer: torch.Size([128, 160, 16, 16])
features.layers.12.layer_scale: torch.Size([128, 160, 16, 16])
features.layers.13.layers.0: torch.Size([128, 160, 16, 16])
features.layers.13.layers.1: torch.Size([128, 160, 16, 16])
features.layers.13.layers.2: torch.Size([128, 640, 16, 16])
features.layers.13.layers.3: torch.Size([128, 640, 16, 16])
features.layers.13.layers.4: torch.Size([128, 640, 16, 16])
features.layers.13.layers.5: torch.Size([128, 640, 16, 16])
features.layers.13.layers.6: torch.Size([128, 640, 16, 16])
features.layers.13.layers.7: torch.Size([128, 640, 16, 16])
features.layers.13.layers.8: torch.Size([128, 160, 16, 16])
features.layers.13.layers.9: torch.Size([128, 160, 16, 16])
features.layers.13.layer_scale: torch.Size([128, 160, 16, 16])
features.layers.14.input_norm: torch.Size([128, 160, 16, 16])
features.layers.14.cpe_dw_conv: torch.Size([128, 160, 16, 16])
features.layers.14.multi_query_attention.query_proj: torch.Size([128, 256, 16, 16])
Error in layer features.layers.14.multi_query_attention.key_dw_conv: Given groups=160, weight of size [160, 1, 3, 3], expected input[128, 256, 16, 16] to have 160 channels, but got 256 channels instead
Error in layer features.layers.14.multi_query_attention.key_dw_norm: running_mean should contain 256 elements not 160
Error in layer features.layers.14.multi_query_attention.key_proj: Given groups=1, weight of size [64, 160, 1, 1], expected input[128, 256, 16, 16] to have 160 channels, but got 256 channels instead
Error in layer features.layers.14.multi_query_attention.value_dw_conv: Given groups=160, weight of size [160, 1, 3, 3], expected input[128, 256, 16, 16] to have 160 channels, but got 256 channels instead
Error in layer features.layers.14.multi_query_attention.value_dw_norm: running_mean should contain 256 elements not 160
Error in layer features.layers.14.multi_query_attention.value_proj: Given groups=1, weight of size [64, 160, 1, 1], expected input[128, 256, 16, 16] to have 160 channels, but got 256 channels instead
features.layers.14.multi_query_attention.output_proj: torch.Size([128, 160, 16, 16])
features.layers.14.multi_query_attention.dropout_layer: torch.Size([128, 160, 16, 16])
features.layers.14.layer_scale: torch.Size([128, 160, 16, 16])
features.layers.15.layers.0: torch.Size([128, 160, 16, 16])
features.layers.15.layers.1: torch.Size([128, 160, 16, 16])
features.layers.15.layers.2: torch.Size([128, 640, 16, 16])
features.layers.15.layers.3: torch.Size([128, 640, 16, 16])
features.layers.15.layers.4: torch.Size([128, 640, 16, 16])
features.layers.15.layers.5: torch.Size([128, 160, 16, 16])
features.layers.15.layers.6: torch.Size([128, 160, 16, 16])
features.layers.15.layer_scale: torch.Size([128, 160, 16, 16])
features.layers.16.layers.0: torch.Size([128, 160, 16, 16])
features.layers.16.layers.1: torch.Size([128, 160, 16, 16])
features.layers.16.layers.2: torch.Size([128, 960, 16, 16])
features.layers.16.layers.3: torch.Size([128, 960, 16, 16])
features.layers.16.layers.4: torch.Size([128, 960, 16, 16])
features.layers.16.layers.5: torch.Size([128, 960, 8, 8])
features.layers.16.layers.6: torch.Size([128, 960, 8, 8])
features.layers.16.layers.7: torch.Size([128, 960, 8, 8])
features.layers.16.layers.8: torch.Size([128, 256, 8, 8])
features.layers.16.layers.9: torch.Size([128, 256, 8, 8])
features.layers.16.layer_scale: torch.Size([128, 256, 8, 8])
features.layers.17.layers.0: torch.Size([128, 256, 8, 8])
features.layers.17.layers.1: torch.Size([128, 256, 8, 8])
features.layers.17.layers.2: torch.Size([128, 1024, 8, 8])
features.layers.17.layers.3: torch.Size([128, 1024, 8, 8])
features.layers.17.layers.4: torch.Size([128, 1024, 8, 8])
features.layers.17.layers.5: torch.Size([128, 1024, 8, 8])
features.layers.17.layers.6: torch.Size([128, 1024, 8, 8])
features.layers.17.layers.7: torch.Size([128, 1024, 8, 8])
features.layers.17.layers.8: torch.Size([128, 256, 8, 8])
features.layers.17.layers.9: torch.Size([128, 256, 8, 8])
features.layers.17.layer_scale: torch.Size([128, 256, 8, 8])
features.layers.18.layers.0: torch.Size([128, 256, 8, 8])
features.layers.18.layers.1: torch.Size([128, 256, 8, 8])
features.layers.18.layers.2: torch.Size([128, 1024, 8, 8])
features.layers.18.layers.3: torch.Size([128, 1024, 8, 8])
features.layers.18.layers.4: torch.Size([128, 1024, 8, 8])
features.layers.18.layers.5: torch.Size([128, 1024, 8, 8])
features.layers.18.layers.6: torch.Size([128, 1024, 8, 8])
features.layers.18.layers.7: torch.Size([128, 1024, 8, 8])
features.layers.18.layers.8: torch.Size([128, 256, 8, 8])
features.layers.18.layers.9: torch.Size([128, 256, 8, 8])
features.layers.18.layer_scale: torch.Size([128, 256, 8, 8])
features.layers.19.layers.0: torch.Size([128, 256, 8, 8])
features.layers.19.layers.1: torch.Size([128, 256, 8, 8])
features.layers.19.layers.2: torch.Size([128, 1024, 8, 8])
features.layers.19.layers.3: torch.Size([128, 1024, 8, 8])
features.layers.19.layers.4: torch.Size([128, 1024, 8, 8])
features.layers.19.layers.5: torch.Size([128, 1024, 8, 8])
features.layers.19.layers.6: torch.Size([128, 1024, 8, 8])
features.layers.19.layers.7: torch.Size([128, 1024, 8, 8])
features.layers.19.layers.8: torch.Size([128, 256, 8, 8])
features.layers.19.layers.9: torch.Size([128, 256, 8, 8])
features.layers.19.layer_scale: torch.Size([128, 256, 8, 8])
features.layers.20.layers.0: torch.Size([128, 512, 8, 8])
features.layers.20.layers.1: torch.Size([128, 512, 8, 8])
features.layers.20.layers.2: torch.Size([128, 512, 8, 8])
features.layers.20.layers.3: torch.Size([128, 256, 8, 8])
features.layers.20.layers.4: torch.Size([128, 256, 8, 8])
features.layers.20.layer_scale: torch.Size([128, 256, 8, 8])
features.layers.21.layers.0: torch.Size([128, 256, 8, 8])
features.layers.21.layers.1: torch.Size([128, 256, 8, 8])
features.layers.21.layers.2: torch.Size([128, 512, 8, 8])
features.layers.21.layers.3: torch.Size([128, 512, 8, 8])
features.layers.21.layers.4: torch.Size([128, 512, 8, 8])
features.layers.21.layers.5: torch.Size([128, 512, 8, 8])
features.layers.21.layers.6: torch.Size([128, 512, 8, 8])
features.layers.21.layers.7: torch.Size([128, 512, 8, 8])
features.layers.21.layers.8: torch.Size([128, 256, 8, 8])
features.layers.21.layers.9: torch.Size([128, 256, 8, 8])
features.layers.21.layer_scale: torch.Size([128, 256, 8, 8])
features.layers.22.layers.0: torch.Size([128, 512, 8, 8])
features.layers.22.layers.1: torch.Size([128, 512, 8, 8])
features.layers.22.layers.2: torch.Size([128, 512, 8, 8])
features.layers.22.layers.3: torch.Size([128, 256, 8, 8])
features.layers.22.layers.4: torch.Size([128, 256, 8, 8])
features.layers.22.layer_scale: torch.Size([128, 256, 8, 8])
features.layers.23.layers.0: torch.Size([128, 1024, 8, 8])
features.layers.23.layers.1: torch.Size([128, 1024, 8, 8])
features.layers.23.layers.2: torch.Size([128, 1024, 8, 8])
features.layers.23.layers.3: torch.Size([128, 256, 8, 8])
features.layers.23.layers.4: torch.Size([128, 256, 8, 8])
features.layers.23.layer_scale: torch.Size([128, 256, 8, 8])
features.layers.24.input_norm: torch.Size([128, 256, 8, 8])
features.layers.24.cpe_dw_conv: torch.Size([128, 256, 8, 8])
features.layers.24.layer_scale: torch.Size([128, 256, 8, 8])
features.layers.25.layers.0: torch.Size([128, 256, 8, 8])
features.layers.25.layers.1: torch.Size([128, 256, 8, 8])
features.layers.25.layers.2: torch.Size([128, 1024, 8, 8])
features.layers.25.layers.3: torch.Size([128, 1024, 8, 8])
features.layers.25.layers.4: torch.Size([128, 1024, 8, 8])
features.layers.25.layers.5: torch.Size([128, 256, 8, 8])
features.layers.25.layers.6: torch.Size([128, 256, 8, 8])
features.layers.25.layer_scale: torch.Size([128, 256, 8, 8])
features.layers.26.input_norm: torch.Size([128, 256, 8, 8])
features.layers.26.cpe_dw_conv: torch.Size([128, 256, 8, 8])
features.layers.26.layer_scale: torch.Size([128, 256, 8, 8])
features.layers.27.layers.0: torch.Size([128, 256, 8, 8])
features.layers.27.layers.1: torch.Size([128, 256, 8, 8])
features.layers.27.layers.2: torch.Size([128, 1024, 8, 8])
features.layers.27.layers.3: torch.Size([128, 1024, 8, 8])
features.layers.27.layers.4: torch.Size([128, 1024, 8, 8])
features.layers.27.layers.5: torch.Size([128, 1024, 8, 8])
features.layers.27.layers.6: torch.Size([128, 1024, 8, 8])
features.layers.27.layers.7: torch.Size([128, 1024, 8, 8])
features.layers.27.layers.8: torch.Size([128, 256, 8, 8])
features.layers.27.layers.9: torch.Size([128, 256, 8, 8])
features.layers.27.layer_scale: torch.Size([128, 256, 8, 8])
features.layers.28.input_norm: torch.Size([128, 256, 8, 8])
features.layers.28.cpe_dw_conv: torch.Size([128, 256, 8, 8])
features.layers.28.layer_scale: torch.Size([128, 256, 8, 8])
features.layers.29.layers.0: torch.Size([128, 256, 8, 8])
features.layers.29.layers.1: torch.Size([128, 256, 8, 8])
features.layers.29.layers.2: torch.Size([128, 1024, 8, 8])
features.layers.29.layers.3: torch.Size([128, 1024, 8, 8])
features.layers.29.layers.4: torch.Size([128, 1024, 8, 8])
features.layers.29.layers.5: torch.Size([128, 256, 8, 8])
features.layers.29.layers.6: torch.Size([128, 256, 8, 8])
features.layers.29.layer_scale: torch.Size([128, 256, 8, 8])
features.layers.30.input_norm: torch.Size([128, 256, 8, 8])
features.layers.30.cpe_dw_conv: torch.Size([128, 256, 8, 8])
features.layers.30.layer_scale: torch.Size([128, 256, 8, 8])
features.layers.31.layers.0: torch.Size([128, 256, 8, 8])
features.layers.31.layers.1: torch.Size([128, 256, 8, 8])
features.layers.31.layers.2: torch.Size([128, 1024, 8, 8])
features.layers.31.layers.3: torch.Size([128, 1024, 8, 8])
features.layers.31.layers.4: torch.Size([128, 1024, 8, 8])
features.layers.31.layers.5: torch.Size([128, 256, 8, 8])
features.layers.31.layers.6: torch.Size([128, 256, 8, 8])
features.layers.31.layer_scale: torch.Size([128, 256, 8, 8])
features.layers.32.conv: torch.Size([128, 960, 8, 8])
features.layers.32.bn: torch.Size([128, 960, 8, 8])
features.layers.32.activation_layer: torch.Size([128, 960, 8, 8])
features.layers.33.pool: torch.Size([128, 960, 1, 1])
features.layers.34.conv: torch.Size([128, 1280, 1, 1])
features.layers.34.bn: torch.Size([128, 1280, 1, 1])
features.layers.34.activation_layer: torch.Size([128, 1280, 1, 1])
classifier.0: torch.Size([128, 1000, 1, 1])
classifier.1: torch.Size([128, 1000])

Final output shape: torch.Size([128, 1000])
==================================================

